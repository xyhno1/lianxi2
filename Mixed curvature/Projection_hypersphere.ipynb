{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef387c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from functools import partial\n",
    "\n",
    "import net\n",
    "from proxy_anchor import dataset\n",
    "from proxy_anchor.utils import calc_recall_at_k\n",
    "from sampler import UniqueClassSempler\n",
    "from proxy_anchor.dataset import CUBirds, SOP, Cars\n",
    "from proxy_anchor.dataset.Inshop import Inshop_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad7bd0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92a2780b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/data/xuyunhao/datasets'\n",
    "ds = 'CUB'\n",
    "num_samples = 2\n",
    "bs = 200\n",
    "lr = 1e-5\n",
    "t = 0.2\n",
    "emb = 512\n",
    "ep = 100\n",
    "local_rank = 0\n",
    "workers = 4\n",
    "optimizer = 'adamw'\n",
    "lr_decay_step = 10\n",
    "lr_decay_gamma = 0.5\n",
    "\n",
    "model =  'resnet34'\n",
    "hyp_c = 0.1\n",
    "clip_r  = 2.3\n",
    "resize = 224\n",
    "crop = 224\n",
    "gpu_id = 4\n",
    "bn_freeze = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f82ab5",
   "metadata": {},
   "source": [
    "# 投影超球模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e56f2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def project(x, *, c=1.0):\n",
    "    c = torch.as_tensor(c).type_as(x)\n",
    "    return _project(x, c)\n",
    "\n",
    "def _project(x, c):\n",
    "    norm = torch.clamp_min(x.norm(dim=-1, keepdim=True, p=2), 1e-5)\n",
    "    maxnorm = (1 - 1e-3) / (c ** 0.5)\n",
    "    cond = norm > maxnorm\n",
    "    projected = x / norm * maxnorm\n",
    "    return torch.where(cond, projected, x)\n",
    "\n",
    "def dexp0(u, *, c=1.0):\n",
    "    c = torch.as_tensor(c).type_as(u)\n",
    "    return _dexp0(u, c)\n",
    "\n",
    "def _dexp0(u, c):\n",
    "    sqrt_c = c ** 0.5\n",
    "    u_norm = torch.clamp_min(u.norm(dim=-1, p=2, keepdim=True), 1e-5)\n",
    "    gamma_1 = torch.tan(sqrt_c * u_norm) * u / (sqrt_c * u_norm)\n",
    "    return gamma_1\n",
    "\n",
    "def _dist_matrix_d(x, y, c):\n",
    "    xy =torch.einsum(\"ij,kj->ik\", (x, y))  # B x C\n",
    "    x2 = x.pow(2).sum(-1, keepdim=True)  # B x 1\n",
    "    y2 = y.pow(2).sum(-1, keepdim=True)  # C x 1\n",
    "    sqrt_c = c ** 0.5\n",
    "    num1 = 2*c*(x2+y2.permute(1, 0)-2*xy)+1e-5\n",
    "    num2 = torch.mul((1+c*x2),(1+c*y2.permute(1, 0)))\n",
    "    return (1/sqrt_c * torch.acos(1-num1/num2))\n",
    "\n",
    "\n",
    "def dist_matrix_d(x, y, c=1.0):\n",
    "    c = torch.as_tensor(c).type_as(x)\n",
    "    return _dist_matrix_d(x, y, c)\n",
    "\n",
    "class ToProjection_hypersphere(nn.Module):\n",
    "    def __init__(self, c, clip_r=None):\n",
    "        super(ToProjection_hypersphere, self).__init__()\n",
    "        self.register_parameter(\"xp\", None)\n",
    "        self.c = c\n",
    "        self.clip_r = clip_r\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.clip_r is not None:\n",
    "            x_norm = torch.norm(x, dim=-1, keepdim=True) + 1e-5\n",
    "            fac =  torch.minimum(\n",
    "                torch.ones_like(x_norm), \n",
    "                self.clip_r / x_norm\n",
    "            )\n",
    "            x = x * fac\n",
    "        return project(dexp0(x, c=self.c), c=self.c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cc339b",
   "metadata": {},
   "source": [
    "# 损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e0296f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(x0, x1, tau, hyp_c):\n",
    "    # x0 and x1 - positive pair\n",
    "    # tau - temperature\n",
    "    # hyp_c - hyperbolic curvature, \"0\" enables sphere mode\n",
    "\n",
    "    dist_f = lambda x, y: -dist_matrix_d(x, y, c=hyp_c)\n",
    "    bsize = x0.shape[0]\n",
    "    target = torch.arange(bsize).cuda()\n",
    "    eye_mask = torch.eye(bsize).cuda() * 1e9\n",
    "    logits00 = dist_f(x0, x0) / tau - eye_mask\n",
    "    logits01 = dist_f(x0, x1) / tau\n",
    "    logits = torch.cat([logits01, logits00], dim=1)\n",
    "    logits -= logits.max(1, keepdim=True)[0].detach()\n",
    "    loss = F.cross_entropy(logits, target)\n",
    "    stats = {\n",
    "        \"logits/min\": logits01.min().item(),\n",
    "        \"logits/mean\": logits01.mean().item(),\n",
    "        \"logits/max\": logits01.max().item(),\n",
    "        \"logits/acc\": (logits01.argmax(-1) == target).float().mean().item(),\n",
    "    }\n",
    "    return loss, stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793d6b10",
   "metadata": {},
   "source": [
    "# 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5fa4780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.nn.init as init\n",
    "import hyptorch.nn as hypnn\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.models import resnet34\n",
    "from torchvision.models import resnet50\n",
    "from torchvision.models import resnet101"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c661e9e",
   "metadata": {},
   "source": [
    "## resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d707471",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resnet18(nn.Module):\n",
    "    def __init__(self,embedding_size, pretrained=True, bn_freeze = True, hyp_c = 0, clip_r = 0):\n",
    "        super(Resnet18, self).__init__()\n",
    "\n",
    "        self.model = resnet18(pretrained)\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hyp_c = hyp_c\n",
    "        self.clip_r = clip_r\n",
    "        self.num_ftrs = self.model.fc.in_features\n",
    "        self.model.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.model.gmp = nn.AdaptiveMaxPool2d(1)\n",
    "        \n",
    "        self.Dlayer = ToProjection_hypersphere(\n",
    "            c=self.hyp_c,\n",
    "            clip_r=self.clip_r,\n",
    "        )\n",
    "        self.model.embedding = nn.Sequential(nn.Linear(self.num_ftrs, self.embedding_size), self.Dlayer)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "        if bn_freeze:\n",
    "            for m in self.model.modules():\n",
    "                if isinstance(m, nn.BatchNorm2d):\n",
    "                    m.eval()\n",
    "                    m.weight.requires_grad_(False)\n",
    "                    m.bias.requires_grad_(False)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model.conv1(x)\n",
    "        x = self.model.bn1(x)\n",
    "        x = self.model.relu(x)\n",
    "        x = self.model.maxpool(x)\n",
    "        x = self.model.layer1(x)\n",
    "        x = self.model.layer2(x)\n",
    "        x = self.model.layer3(x)\n",
    "        x = self.model.layer4(x)\n",
    "\n",
    "        avg_x = self.model.gap(x)\n",
    "        max_x = self.model.gmp(x)\n",
    "\n",
    "        x = max_x + avg_x\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x_d = self.model.embedding(x)\n",
    "        return x_d\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        init.kaiming_normal_(self.model.embedding[0].weight, mode='fan_out')\n",
    "        init.constant_(self.model.embedding[0].bias, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ceb6a75",
   "metadata": {},
   "source": [
    "## resnet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "884c1798",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resnet34(nn.Module):\n",
    "    def __init__(self,embedding_size, pretrained=True, bn_freeze = True, hyp_c = 0, clip_r = 0):\n",
    "        super(Resnet34, self).__init__()\n",
    "\n",
    "        self.model = resnet34(pretrained)\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hyp_c = hyp_c\n",
    "        self.clip_r = clip_r\n",
    "        self.num_ftrs = self.model.fc.in_features\n",
    "        self.model.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.model.gmp = nn.AdaptiveMaxPool2d(1)\n",
    "        \n",
    "        self.Dlayer = ToProjection_hypersphere(\n",
    "            c=self.hyp_c,\n",
    "            clip_r=self.clip_r,\n",
    "        )\n",
    "        self.model.embedding = nn.Sequential(nn.Linear(self.num_ftrs, self.embedding_size), self.Dlayer)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "        if bn_freeze:\n",
    "            for m in self.model.modules():\n",
    "                if isinstance(m, nn.BatchNorm2d):\n",
    "                    m.eval()\n",
    "                    m.weight.requires_grad_(False)\n",
    "                    m.bias.requires_grad_(False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model.conv1(x)\n",
    "        x = self.model.bn1(x)\n",
    "        x = self.model.relu(x)\n",
    "        x = self.model.maxpool(x)\n",
    "        x = self.model.layer1(x)\n",
    "        x = self.model.layer2(x)\n",
    "        x = self.model.layer3(x)\n",
    "        x = self.model.layer4(x)\n",
    "\n",
    "        avg_x = self.model.gap(x)\n",
    "        max_x = self.model.gmp(x)\n",
    "\n",
    "        x = avg_x + max_x\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x_p = self.model.embedding(x)\n",
    "        return x_p\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        init.kaiming_normal_(self.model.embedding[0].weight, mode='fan_out')\n",
    "        init.constant_(self.model.embedding[0].bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14084863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(get_emb_f, ds_name, hyp_c):\n",
    "    emb_head = get_emb_f(ds_type=\"eval\")\n",
    "    recall_head = get_recall(*emb_head, ds_name, hyp_c)\n",
    "    return recall_head\n",
    "\n",
    "def get_recall(d, y, ds_name, hyp_c):\n",
    "    if ds_name == \"CUB\" or ds_name == \"Cars\":\n",
    "        k_list = [1, 2, 4, 8, 16, 32]\n",
    "    elif ds_name == \"SOP\":\n",
    "        k_list = [1, 10, 100, 1000]\n",
    "\n",
    "    dist_m = torch.empty(len(d), len(d), device=\"cuda\")\n",
    "    for i in range(len(d)):\n",
    "        dist_m[i : i + 1] = -dist_matrix_d(d[i : i + 1], d, hyp_c)\n",
    "\n",
    "    y_cur = y[dist_m.topk(1 + max(k_list), largest=True)[1][:, 1:]]\n",
    "    y = y.cpu()\n",
    "    y_cur = y_cur.float().cpu()\n",
    "    recall = [calc_recall_at_k(y, y_cur, k) for k in k_list]\n",
    "    print(recall)\n",
    "    return recall[0]\n",
    "\n",
    "def get_emb(\n",
    "    model,\n",
    "    ds,\n",
    "    path,\n",
    "    ds_type=\"eval\",\n",
    "    world_size=1,\n",
    "    num_workers=8,\n",
    "):\n",
    "    eval_tr = dataset.utils.make_transform(\n",
    "        is_train = True, \n",
    "        is_inception = (model == 'bn_inception')\n",
    "    )\n",
    "    ds_eval = ds(path, ds_type, eval_tr)\n",
    "    if world_size == 1:\n",
    "        sampler = None\n",
    "    else:\n",
    "        sampler = torch.utils.data.distributed.DistributedSampler(ds_eval)\n",
    "    dl_eval = DataLoader(\n",
    "        dataset=ds_eval,\n",
    "        batch_size=100,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        sampler=sampler,\n",
    "    )\n",
    "    model.eval()\n",
    "    d, y = eval_dataset(model, dl_eval)\n",
    "    y = y.cuda()\n",
    "    model.train()\n",
    "    return d, y\n",
    "\n",
    "def eval_dataset(model, dl):\n",
    "    all_xd, all_y = [], []\n",
    "    for x, y in dl:\n",
    "        with torch.no_grad():\n",
    "            x = x.cuda(non_blocking=True)\n",
    "            d= model(x)\n",
    "            all_xd.append(d)\n",
    "        all_y.append(y)\n",
    "    return torch.cat(all_xd), torch.cat(all_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2678d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 100 epochs.\n",
      "[0.4176232275489534, 0.5406819716407832, 0.6561444969615124, 0.762322754895341, 0.8529709655638082, 0.9098582039162728]\n",
      "The recall before train:  0.4176232275489534\n",
      "[0.42133693450371373, 0.5482781904118839, 0.661883862255233, 0.7694125590817016, 0.8511141120864281, 0.9100270087778528]\n",
      "epoch: 0 recall:  0.42133693450371373\n",
      "best epoch: 0 best recall:  0.42133693450371373\n",
      "[0.4289331532748143, 0.5518230925050641, 0.6711681296421337, 0.7743079000675219, 0.8607359891964889, 0.9177920324105334]\n",
      "epoch: 1 recall:  0.4289331532748143\n",
      "best epoch: 1 best recall:  0.4289331532748143\n",
      "[0.425050641458474, 0.5570560432140446, 0.6694800810263336, 0.7775151924375422, 0.8590479405806887, 0.9167792032410533]\n",
      "epoch: 2 recall:  0.425050641458474\n",
      "best epoch: 1 best recall:  0.4289331532748143\n",
      "[0.43416610398379474, 0.5607697501688048, 0.675557056043214, 0.7842673869007427, 0.8637744767049291, 0.9196488858879136]\n",
      "epoch: 3 recall:  0.43416610398379474\n",
      "best epoch: 3 best recall:  0.43416610398379474\n",
      "[0.4265698852126941, 0.5558744091829845, 0.6735313977042539, 0.7771775827143822, 0.8582039162727887, 0.9144159351789332]\n",
      "epoch: 4 recall:  0.4265698852126941\n",
      "best epoch: 3 best recall:  0.43416610398379474\n",
      "[0.43433490884537473, 0.5540175557056043, 0.6709993247805537, 0.7704253882511817, 0.8538149898717083, 0.9096893990546928]\n",
      "epoch: 5 recall:  0.43433490884537473\n",
      "best epoch: 5 best recall:  0.43433490884537473\n",
      "[0.4410871033085753, 0.5643146522619852, 0.6779203241053342, 0.7840985820391627, 0.8607359891964889, 0.9139095205941931]\n",
      "epoch: 6 recall:  0.4410871033085753\n",
      "best epoch: 6 best recall:  0.4410871033085753\n",
      "[0.4453072248480756, 0.5666779203241054, 0.6792707629979743, 0.7835921674544227, 0.8568534773801485, 0.9164415935178933]\n",
      "epoch: 7 recall:  0.4453072248480756\n",
      "best epoch: 7 best recall:  0.4453072248480756\n",
      "[0.4355165428764348, 0.5638082376772451, 0.6762322754895341, 0.7758271438217421, 0.8578663065496286, 0.9162727886563133]\n",
      "epoch: 8 recall:  0.4355165428764348\n",
      "best epoch: 7 best recall:  0.4453072248480756\n",
      "[0.43197164078325456, 0.5582376772451046, 0.6701553004726536, 0.7776839972991222, 0.8543214044564483, 0.9155975692099932]\n",
      "epoch: 9 recall:  0.43197164078325456\n",
      "best epoch: 7 best recall:  0.4453072248480756\n",
      "[0.44074949358541526, 0.5678595543551654, 0.6780891289669142, 0.7775151924375422, 0.8531397704253882, 0.9107022282241729]\n",
      "epoch: 10 recall:  0.44074949358541526\n",
      "best epoch: 7 best recall:  0.4453072248480756\n",
      "[0.44952734638757597, 0.575793382849426, 0.6900742741390952, 0.7862930452397029, 0.862761647535449, 0.9171168129642133]\n",
      "epoch: 11 recall:  0.44952734638757597\n",
      "best epoch: 11 best recall:  0.44952734638757597\n",
      "[0.4529034436191762, 0.5722484807562458, 0.6892302498311952, 0.7829169480081026, 0.8565158676569885, 0.9103646185010128]\n",
      "epoch: 12 recall:  0.4529034436191762\n",
      "best epoch: 12 best recall:  0.4529034436191762\n",
      "[0.43686698176907496, 0.5560432140445645, 0.6709993247805537, 0.774983119513842, 0.8568534773801485, 0.9140783254557732]\n",
      "epoch: 13 recall:  0.43686698176907496\n",
      "best epoch: 12 best recall:  0.4529034436191762\n",
      "[0.4463200540175557, 0.5639770425388251, 0.6737002025658338, 0.7744767049291019, 0.8534773801485482, 0.9095205941931127]\n",
      "epoch: 14 recall:  0.4463200540175557\n",
      "best epoch: 12 best recall:  0.4529034436191762\n",
      "[0.4426063470627954, 0.5570560432140446, 0.6824780553679946, 0.7835921674544227, 0.8612424037812288, 0.9144159351789332]\n",
      "epoch: 15 recall:  0.4426063470627954\n",
      "best epoch: 12 best recall:  0.4529034436191762\n",
      "[0.45070898041863605, 0.5673531397704253, 0.6834908845374746, 0.7768399729912221, 0.8603983794733289, 0.9150911546252533]\n",
      "epoch: 16 recall:  0.45070898041863605\n",
      "best epoch: 12 best recall:  0.4529034436191762\n",
      "[0.44564483457123566, 0.5654962862930453, 0.6861917623227549, 0.7846049966239028, 0.8637744767049291, 0.9154287643484132]\n",
      "epoch: 17 recall:  0.44564483457123566\n",
      "best epoch: 12 best recall:  0.4529034436191762\n",
      "[0.4410871033085753, 0.562288993923025, 0.6823092505064146, 0.7832545577312626, 0.8600607697501688, 0.9177920324105334]\n",
      "epoch: 18 recall:  0.4410871033085753\n",
      "best epoch: 12 best recall:  0.4529034436191762\n",
      "[0.45054017555705606, 0.5798446995273464, 0.6905806887238353, 0.787474679270763, 0.8602295746117489, 0.9113774476704929]\n",
      "epoch: 19 recall:  0.45054017555705606\n",
      "best epoch: 12 best recall:  0.4529034436191762\n",
      "[0.4432815665091155, 0.5551991897366644, 0.6682984469952734, 0.7705941931127617, 0.8531397704253882, 0.912559081701553]\n",
      "epoch: 20 recall:  0.4432815665091155\n",
      "best epoch: 12 best recall:  0.4529034436191762\n",
      "[0.4486833220796759, 0.5654962862930453, 0.6816340310600946, 0.7761647535449021, 0.8561782579338285, 0.9147535449020932]\n",
      "epoch: 21 recall:  0.4486833220796759\n",
      "best epoch: 12 best recall:  0.4529034436191762\n",
      "[0.45054017555705606, 0.5769750168804861, 0.6934503713706954, 0.7879810938555031, 0.8600607697501688, 0.912221471978393]\n",
      "epoch: 22 recall:  0.45054017555705606\n",
      "best epoch: 12 best recall:  0.4529034436191762\n",
      "[0.43923024983119513, 0.5604321404456448, 0.6747130317353139, 0.7781904118838623, 0.8558406482106685, 0.9088453747467927]\n",
      "epoch: 23 recall:  0.43923024983119513\n",
      "best epoch: 12 best recall:  0.4529034436191762\n",
      "[0.4579675894665766, 0.5796758946657664, 0.687373396353815, 0.7851114112086428, 0.8652937204591492, 0.9179608372721134]\n",
      "epoch: 24 recall:  0.4579675894665766\n",
      "best epoch: 24 best recall:  0.4579675894665766\n",
      "[0.449864956110736, 0.5698852126941256, 0.687879810938555, 0.7851114112086428, 0.862930452397029, 0.9162727886563133]\n",
      "epoch: 25 recall:  0.449864956110736\n",
      "best epoch: 24 best recall:  0.4579675894665766\n",
      "[0.4436191762322755, 0.5638082376772451, 0.6850101282916948, 0.7805536799459825, 0.8570222822417286, 0.9149223497636731]\n",
      "epoch: 26 recall:  0.4436191762322755\n",
      "best epoch: 24 best recall:  0.4579675894665766\n",
      "[0.44058068872383527, 0.5582376772451046, 0.675050641458474, 0.7771775827143822, 0.8592167454422688, 0.9145847400405132]\n",
      "epoch: 27 recall:  0.44058068872383527\n",
      "best epoch: 24 best recall:  0.4579675894665766\n",
      "[0.44969615124915596, 0.5710668467251857, 0.6809588116137745, 0.7739702903443619, 0.8592167454422688, 0.9155975692099932]\n",
      "epoch: 28 recall:  0.44969615124915596\n",
      "best epoch: 24 best recall:  0.4579675894665766\n",
      "[0.4540850776502363, 0.5665091154625254, 0.6811276164753545, 0.7763335584064821, 0.8571910871033086, 0.9130654962862931]\n",
      "epoch: 29 recall:  0.4540850776502363\n",
      "best epoch: 24 best recall:  0.4579675894665766\n",
      "[0.4397366644159352, 0.5607697501688048, 0.6779203241053342, 0.7776839972991222, 0.8474004051316678, 0.9061444969615124]\n",
      "epoch: 30 recall:  0.4397366644159352\n",
      "best epoch: 24 best recall:  0.4579675894665766\n",
      "[0.4471640783254558, 0.5675219446320054, 0.6804523970290345, 0.7778528021607022, 0.8568534773801485, 0.9157663740715732]\n",
      "epoch: 31 recall:  0.4471640783254558\n",
      "best epoch: 24 best recall:  0.4579675894665766\n",
      "[0.4532410533423363, 0.5714044564483457, 0.6885550303848751, 0.7822417285617825, 0.8622552329507089, 0.9145847400405132]\n",
      "epoch: 32 recall:  0.4532410533423363\n",
      "best epoch: 24 best recall:  0.4579675894665766\n",
      "[0.44682646860229575, 0.5695476029709655, 0.6775827143821742, 0.7758271438217421, 0.8560094530722485, 0.9110398379473329]\n",
      "epoch: 33 recall:  0.44682646860229575\n",
      "best epoch: 24 best recall:  0.4579675894665766\n",
      "[0.4552667116812964, 0.5724172856178258, 0.6833220796758946, 0.7881498987170831, 0.8641120864280891, 0.9166103983794733]\n",
      "epoch: 34 recall:  0.4552667116812964\n",
      "best epoch: 24 best recall:  0.4579675894665766\n",
      "[0.44800810263335583, 0.5675219446320054, 0.6769074949358541, 0.775320729237002, 0.8585415259959487, 0.9101958136394328]\n",
      "epoch: 35 recall:  0.44800810263335583\n",
      "best epoch: 24 best recall:  0.4579675894665766\n",
      "[0.43315327481431465, 0.5617825793382849, 0.675557056043214, 0.7763335584064821, 0.8595543551654288, 0.9171168129642133]\n",
      "epoch: 36 recall:  0.43315327481431465\n",
      "best epoch: 24 best recall:  0.4579675894665766\n",
      "[0.4571235651586766, 0.5737677245104659, 0.6899054692775152, 0.7808912896691425, 0.8585415259959487, 0.9149223497636731]\n",
      "epoch: 37 recall:  0.4571235651586766\n",
      "best epoch: 24 best recall:  0.4579675894665766\n",
      "[0.43602295746117486, 0.5673531397704253, 0.6826468602295747, 0.7805536799459825, 0.8556718433490884, 0.9101958136394328]\n",
      "epoch: 38 recall:  0.43602295746117486\n",
      "best epoch: 24 best recall:  0.4579675894665766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4523970290344362, 0.5717420661715057, 0.6865293720459149, 0.7859554355165429, 0.8539837947332883, 0.9088453747467927]\n",
      "epoch: 39 recall:  0.4523970290344362\n",
      "best epoch: 24 best recall:  0.4579675894665766\n",
      "[0.4404118838622552, 0.5639770425388251, 0.6764010803511141, 0.775489534098582, 0.8573598919648886, 0.911883862255233]\n",
      "epoch: 40 recall:  0.4404118838622552\n",
      "best epoch: 24 best recall:  0.4579675894665766\n",
      "[0.4549291019581364, 0.5666779203241054, 0.6853477380148548, 0.7800472653612424, 0.8590479405806887, 0.9150911546252533]\n",
      "epoch: 41 recall:  0.4549291019581364\n",
      "best epoch: 24 best recall:  0.4579675894665766\n",
      "[0.4426063470627954, 0.5631330182309251, 0.674206617150574, 0.7709318028359217, 0.8512829169480081, 0.9061444969615124]\n",
      "epoch: 42 recall:  0.4426063470627954\n",
      "best epoch: 24 best recall:  0.4579675894665766\n",
      "[0.4485145172180959, 0.5700540175557056, 0.6843349088453747, 0.7807224848075625, 0.8590479405806887, 0.9134031060094531]\n",
      "epoch: 43 recall:  0.4485145172180959\n",
      "best epoch: 24 best recall:  0.4579675894665766\n",
      "[0.44682646860229575, 0.5665091154625254, 0.6807900067521945, 0.7780216070222823, 0.8539837947332883, 0.9161039837947332]\n",
      "epoch: 44 recall:  0.44682646860229575\n",
      "best epoch: 24 best recall:  0.4579675894665766\n",
      "[0.44564483457123566, 0.5670155300472653, 0.6823092505064146, 0.7805536799459825, 0.8560094530722485, 0.9147535449020932]\n",
      "epoch: 45 recall:  0.44564483457123566\n",
      "best epoch: 24 best recall:  0.4579675894665766\n",
      "[0.44395678595543553, 0.5649898717083052, 0.6856853477380148, 0.7856178257933828, 0.8612424037812288, 0.9159351789331532]\n",
      "epoch: 46 recall:  0.44395678595543553\n",
      "best epoch: 24 best recall:  0.4579675894665766\n",
      "[0.4512153950033761, 0.5703916272788656, 0.6787643484132343, 0.7778528021607022, 0.8575286968264686, 0.9154287643484132]\n",
      "epoch: 47 recall:  0.4512153950033761\n",
      "best epoch: 24 best recall:  0.4579675894665766\n",
      "[0.44463200540175557, 0.5710668467251857, 0.6829844699527347, 0.7834233625928426, 0.8622552329507089, 0.9147535449020932]\n",
      "epoch: 48 recall:  0.44463200540175557\n",
      "best epoch: 24 best recall:  0.4579675894665766\n",
      "[0.4412559081701553, 0.5680283592167454, 0.6845037137069547, 0.7819041188386225, 0.8595543551654288, 0.9103646185010128]\n",
      "epoch: 49 recall:  0.4412559081701553\n",
      "best epoch: 24 best recall:  0.4579675894665766\n",
      "[0.4426063470627954, 0.5683659689399054, 0.6863605671843349, 0.7857866306549629, 0.8641120864280891, 0.9169480081026333]\n",
      "epoch: 50 recall:  0.4426063470627954\n",
      "best epoch: 24 best recall:  0.4579675894665766\n",
      "[0.44058068872383527, 0.5714044564483457, 0.687035786630655, 0.7803848750844025, 0.8580351114112087, 0.9113774476704929]\n",
      "epoch: 51 recall:  0.44058068872383527\n",
      "best epoch: 24 best recall:  0.4579675894665766\n",
      "[0.45476029709655635, 0.5707292370020257, 0.6812964213369345, 0.7765023632680621, 0.8538149898717083, 0.9088453747467927]\n",
      "epoch: 52 recall:  0.45476029709655635\n",
      "best epoch: 24 best recall:  0.4579675894665766\n",
      "[0.437373396353815, 0.5592505064145847, 0.6814652261985145, 0.7802160702228225, 0.849932478055368, 0.9044564483457124]\n",
      "epoch: 53 recall:  0.437373396353815\n",
      "best epoch: 24 best recall:  0.4579675894665766\n",
      "[0.4463200540175557, 0.5685347738014855, 0.6833220796758946, 0.7829169480081026, 0.8541525995948683, 0.9108710330857529]\n",
      "epoch: 54 recall:  0.4463200540175557\n",
      "best epoch: 24 best recall:  0.4579675894665766\n",
      "[0.4523970290344362, 0.5675219446320054, 0.6888926401080351, 0.7805536799459825, 0.8558406482106685, 0.9117150573936529]\n",
      "epoch: 55 recall:  0.4523970290344362\n",
      "best epoch: 24 best recall:  0.4579675894665766\n",
      "[0.4550979068197164, 0.575793382849426, 0.687204591492235, 0.7824105334233626, 0.8597231600270088, 0.9166103983794733]\n",
      "epoch: 56 recall:  0.4550979068197164\n",
      "best epoch: 24 best recall:  0.4579675894665766\n",
      "[0.4475016880486158, 0.5768062120189061, 0.6885550303848751, 0.7781904118838623, 0.8561782579338285, 0.9135719108710331]\n",
      "epoch: 57 recall:  0.4475016880486158\n",
      "best epoch: 24 best recall:  0.4579675894665766\n",
      "[0.4491897366644159, 0.5729237002025659, 0.6863605671843349, 0.7847738014854828, 0.8592167454422688, 0.9152599594868333]\n",
      "epoch: 58 recall:  0.4491897366644159\n",
      "best epoch: 24 best recall:  0.4579675894665766\n",
      "[0.4529034436191762, 0.5779878460499662, 0.6856853477380148, 0.7820729237002025, 0.8573598919648886, 0.9135719108710331]\n",
      "epoch: 59 recall:  0.4529034436191762\n",
      "best epoch: 24 best recall:  0.4579675894665766\n",
      "[0.44952734638757597, 0.5714044564483457, 0.6887238352464551, 0.7837609723160027, 0.8598919648885888, 0.9147535449020932]\n",
      "epoch: 60 recall:  0.44952734638757597\n",
      "best epoch: 24 best recall:  0.4579675894665766\n",
      "[0.4463200540175557, 0.5602633355840648, 0.6814652261985145, 0.7837609723160027, 0.8609047940580689, 0.9182984469952734]\n",
      "epoch: 61 recall:  0.4463200540175557\n",
      "best epoch: 24 best recall:  0.4579675894665766\n",
      "[0.4648885887913572, 0.5838960162052668, 0.6946320054017556, 0.787305874409183, 0.8573598919648886, 0.9147535449020932]\n",
      "epoch: 62 recall:  0.4648885887913572\n",
      "best epoch: 62 best recall:  0.4648885887913572\n",
      "[0.44463200540175557, 0.5673531397704253, 0.6770762997974341, 0.7743079000675219, 0.8538149898717083, 0.9093517893315327]\n",
      "epoch: 63 recall:  0.44463200540175557\n",
      "best epoch: 62 best recall:  0.4648885887913572\n",
      "[0.4510465901417961, 0.5717420661715057, 0.6865293720459149, 0.7803848750844025, 0.8587103308575287, 0.9181296421336934]\n",
      "epoch: 64 recall:  0.4510465901417961\n",
      "best epoch: 62 best recall:  0.4648885887913572\n",
      "[0.4699527346387576, 0.5840648210668468, 0.6924375422012155, 0.7905131667792032, 0.8603983794733289, 0.9172856178257934]\n",
      "epoch: 65 recall:  0.4699527346387576\n",
      "best epoch: 65 best recall:  0.4699527346387576\n",
      "[0.4402430790006752, 0.5646522619851452, 0.6794395678595544, 0.7793720459149224, 0.8541525995948683, 0.9066509115462525]\n",
      "epoch: 66 recall:  0.4402430790006752\n",
      "best epoch: 65 best recall:  0.4699527346387576\n",
      "[0.4561107359891965, 0.5705604321404456, 0.6821404456448346, 0.7864618501012829, 0.8598919648885888, 0.9137407157326131]\n",
      "epoch: 67 recall:  0.4561107359891965\n",
      "best epoch: 65 best recall:  0.4699527346387576\n",
      "[0.4525658338960162, 0.5614449696151249, 0.6769074949358541, 0.775320729237002, 0.8566846725185685, 0.9145847400405132]\n",
      "epoch: 68 recall:  0.4525658338960162\n",
      "best epoch: 65 best recall:  0.4699527346387576\n",
      "[0.45425388251181636, 0.5666779203241054, 0.6839972991222147, 0.7800472653612424, 0.8561782579338285, 0.9150911546252533]\n",
      "epoch: 69 recall:  0.45425388251181636\n",
      "best epoch: 65 best recall:  0.4699527346387576\n",
      "[0.4412559081701553, 0.562626603646185, 0.6802835921674544, 0.7837609723160027, 0.8658001350438893, 0.9201553004726536]\n",
      "epoch: 70 recall:  0.4412559081701553\n",
      "best epoch: 65 best recall:  0.4699527346387576\n",
      "[0.4441255908170155, 0.5683659689399054, 0.6806212018906145, 0.7763335584064821, 0.8565158676569885, 0.9132343011478731]\n",
      "epoch: 71 recall:  0.4441255908170155\n",
      "best epoch: 65 best recall:  0.4699527346387576\n",
      "[0.450202565833896, 0.5717420661715057, 0.6904118838622553, 0.7824105334233626, 0.8565158676569885, 0.9112086428089129]\n",
      "epoch: 72 recall:  0.450202565833896\n",
      "best epoch: 65 best recall:  0.4699527346387576\n",
      "[0.4512153950033761, 0.5631330182309251, 0.6772451046590142, 0.7726198514517219, 0.8487508440243079, 0.9069885212694125]\n",
      "epoch: 73 recall:  0.4512153950033761\n",
      "best epoch: 65 best recall:  0.4699527346387576\n",
      "[0.44800810263335583, 0.5663403106009454, 0.6787643484132343, 0.7771775827143822, 0.8536461850101283, 0.9144159351789332]\n",
      "epoch: 74 recall:  0.44800810263335583\n",
      "best epoch: 65 best recall:  0.4699527346387576\n",
      "[0.4441255908170155, 0.5665091154625254, 0.6821404456448346, 0.7771775827143822, 0.850270087778528, 0.912559081701553]\n",
      "epoch: 75 recall:  0.4441255908170155\n",
      "best epoch: 65 best recall:  0.4699527346387576\n",
      "[0.44074949358541526, 0.5688723835246455, 0.6838284942606347, 0.7837609723160027, 0.8598919648885888, 0.9157663740715732]\n",
      "epoch: 76 recall:  0.44074949358541526\n",
      "best epoch: 65 best recall:  0.4699527346387576\n",
      "[0.4532410533423363, 0.5766374071573261, 0.6893990546927752, 0.7866306549628629, 0.8602295746117489, 0.9189736664415935]\n",
      "epoch: 77 recall:  0.4532410533423363\n",
      "best epoch: 65 best recall:  0.4699527346387576\n",
      "[0.4464888588791357, 0.5720796758946658, 0.6866981769074949, 0.7829169480081026, 0.8566846725185685, 0.9108710330857529]\n",
      "epoch: 78 recall:  0.4464888588791357\n",
      "best epoch: 65 best recall:  0.4699527346387576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4520594193112762, 0.5729237002025659, 0.6816340310600946, 0.7830857528696826, 0.861411208642809, 0.9179608372721134]\n",
      "epoch: 79 recall:  0.4520594193112762\n",
      "best epoch: 65 best recall:  0.4699527346387576\n",
      "[0.4529034436191762, 0.5739365293720459, 0.6826468602295747, 0.775320729237002, 0.8490884537474679, 0.9051316677920324]\n",
      "epoch: 80 recall:  0.4529034436191762\n",
      "best epoch: 65 best recall:  0.4699527346387576\n",
      "[0.4523970290344362, 0.5658338960162053, 0.6735313977042539, 0.7773463875759622, 0.8583727211343687, 0.9154287643484132]\n",
      "epoch: 81 recall:  0.4523970290344362\n",
      "best epoch: 65 best recall:  0.4699527346387576\n",
      "[0.4513841998649561, 0.5735989196488859, 0.6851789331532748, 0.7891627278865632, 0.8644496961512491, 0.9135719108710331]\n",
      "epoch: 82 recall:  0.4513841998649561\n",
      "best epoch: 65 best recall:  0.4699527346387576\n",
      "[0.44176232275489535, 0.562626603646185, 0.6785955435516543, 0.7741390952059419, 0.8487508440243079, 0.9093517893315327]\n",
      "epoch: 83 recall:  0.44176232275489535\n",
      "best epoch: 65 best recall:  0.4699527346387576\n",
      "[0.44665766374071575, 0.5729237002025659, 0.6828156650911547, 0.7807224848075625, 0.8605671843349089, 0.9105334233625928]\n",
      "epoch: 84 recall:  0.44665766374071575\n",
      "best epoch: 65 best recall:  0.4699527346387576\n",
      "[0.437373396353815, 0.5594193112761647, 0.6711681296421337, 0.7766711681296421, 0.8588791357191087, 0.9139095205941931]\n",
      "epoch: 85 recall:  0.437373396353815\n",
      "best epoch: 65 best recall:  0.4699527346387576\n",
      "[0.44395678595543553, 0.5681971640783254, 0.6861917623227549, 0.7891627278865632, 0.862592842673869, 0.9181296421336934]\n",
      "epoch: 86 recall:  0.44395678595543553\n",
      "best epoch: 65 best recall:  0.4699527346387576\n",
      "[0.4441255908170155, 0.5607697501688048, 0.674206617150574, 0.7771775827143822, 0.8573598919648886, 0.9134031060094531]\n",
      "epoch: 87 recall:  0.4441255908170155\n",
      "best epoch: 65 best recall:  0.4699527346387576\n",
      "[0.4572923700202566, 0.5717420661715057, 0.6839972991222147, 0.7785280216070223, 0.8549966239027684, 0.9137407157326131]\n",
      "epoch: 88 recall:  0.4572923700202566\n",
      "best epoch: 65 best recall:  0.4699527346387576\n",
      "[0.4449696151249156, 0.5663403106009454, 0.6801147873058744, 0.7736326806212019, 0.8526333558406483, 0.9073261309925725]\n",
      "epoch: 89 recall:  0.4449696151249156\n",
      "best epoch: 65 best recall:  0.4699527346387576\n",
      "[0.437711006076975, 0.562288993923025, 0.6787643484132343, 0.775489534098582, 0.8517893315327482, 0.9105334233625928]\n",
      "epoch: 90 recall:  0.437711006076975\n",
      "best epoch: 65 best recall:  0.4699527346387576\n",
      "[0.4523970290344362, 0.5681971640783254, 0.6851789331532748, 0.7776839972991222, 0.8587103308575287, 0.9135719108710331]\n",
      "epoch: 91 recall:  0.4523970290344362\n",
      "best epoch: 65 best recall:  0.4699527346387576\n",
      "[0.44969615124915596, 0.574274139095206, 0.6833220796758946, 0.7770087778528022, 0.8609047940580689, 0.9157663740715732]\n",
      "epoch: 92 recall:  0.44969615124915596\n",
      "best epoch: 65 best recall:  0.4699527346387576\n",
      "[0.43112761647535447, 0.5600945307224848, 0.6748818365968939, 0.7746455097906819, 0.8566846725185685, 0.9139095205941931]\n",
      "epoch: 93 recall:  0.43112761647535447\n",
      "best epoch: 65 best recall:  0.4699527346387576\n",
      "[0.4520594193112762, 0.5735989196488859, 0.6836596893990546, 0.7898379473328832, 0.861917623227549, 0.912390276839973]\n",
      "epoch: 94 recall:  0.4520594193112762\n",
      "best epoch: 65 best recall:  0.4699527346387576\n",
      "[0.4449696151249156, 0.562288993923025, 0.6733625928426739, 0.7797096556380824, 0.8524645509790681, 0.9113774476704929]\n",
      "epoch: 95 recall:  0.4449696151249156\n",
      "best epoch: 65 best recall:  0.4699527346387576\n",
      "[0.4463200540175557, 0.5646522619851452, 0.6821404456448346, 0.7817353139770425, 0.8568534773801485, 0.911883862255233]\n",
      "epoch: 96 recall:  0.4463200540175557\n",
      "best epoch: 65 best recall:  0.4699527346387576\n",
      "[0.4464888588791357, 0.5648210668467252, 0.6735313977042539, 0.7738014854827819, 0.8546590141796083, 0.9096893990546928]\n",
      "epoch: 97 recall:  0.4464888588791357\n",
      "best epoch: 65 best recall:  0.4699527346387576\n",
      "[0.4599932478055368, 0.5764686022957461, 0.6828156650911547, 0.7835921674544227, 0.8607359891964889, 0.9150911546252533]\n",
      "epoch: 98 recall:  0.4599932478055368\n",
      "best epoch: 65 best recall:  0.4699527346387576\n",
      "[0.43889264010803514, 0.5653274814314653, 0.6855165428764348, 0.7788656313301823, 0.8563470627954085, 0.9176232275489534]\n",
      "epoch: 99 recall:  0.43889264010803514\n",
      "best epoch: 65 best recall:  0.4699527346387576\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.set_device(gpu_id)\n",
    "world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "\n",
    "train_tr = dataset.utils.make_transform(\n",
    "    is_train = True, \n",
    "    is_inception = (model == 'bn_inception')\n",
    ")\n",
    "\n",
    "ds_list = {\"CUB\": CUBirds, \"SOP\": SOP, \"Cars\": Cars, \"Inshop\": Inshop_Dataset}\n",
    "ds_class = ds_list[ds]\n",
    "ds_train = ds_class(path, \"train\", train_tr)\n",
    "\n",
    "sampler = UniqueClassSempler(\n",
    "    ds_train.ys, num_samples, local_rank, world_size\n",
    ")\n",
    "dl_train = DataLoader(\n",
    "    dataset=ds_train,\n",
    "    sampler=sampler,\n",
    "    batch_size=bs,\n",
    "    num_workers=workers,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "if model.find('resnet18')+1:\n",
    "    model = Resnet18(embedding_size=emb, pretrained=True, bn_freeze = bn_freeze,hyp_c = hyp_c, clip_r = clip_r).cuda().train() \n",
    "elif model.find('resnet34')+1:\n",
    "    model = Resnet34(embedding_size=emb, pretrained=True, bn_freeze = bn_freeze,hyp_c = hyp_c, clip_r = clip_r).cuda().train() \n",
    "elif model.find('resnet50')+1:\n",
    "    model = Resnet50(embedding_size=emb, pretrained=True, bn_freeze = bn_freeze,hyp_c = hyp_c, clip_r = clip_r).cuda().train() \n",
    "elif model.find('resnet101')+1:\n",
    "    model = Resnet101(embedding_size=emb, pretrained=True, bn_freeze = bn_freeze,hyp_c = hyp_c, clip_r = clip_r).cuda().train() \n",
    "\n",
    "loss_f = partial(contrastive_loss, tau=t, hyp_c= hyp_c)\n",
    "\n",
    "get_emb_f = partial(\n",
    "    get_emb,\n",
    "    model=model,\n",
    "    ds=ds_class,\n",
    "    path=path,\n",
    "    num_workers=workers,\n",
    "    world_size=world_size,\n",
    ")\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=lr_decay_step, gamma = lr_decay_gamma)\n",
    "print(\"Training for {} epochs.\".format(ep))\n",
    "\n",
    "r0= evaluate(get_emb_f, ds, hyp_c=hyp_c)\n",
    "print(\"The recall before train: \", r0)\n",
    "\n",
    "losses_list = []\n",
    "best_recall= 0\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(0, ep):\n",
    "    model.train()\n",
    "    if bn_freeze:\n",
    "        modules = model.model.modules()\n",
    "        for m in modules: \n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                m.eval()\n",
    "\n",
    "    losses_per_epoch = []\n",
    "    sampler.set_epoch(epoch)\n",
    "    stats_ep = []\n",
    "    for x, y in dl_train:\n",
    "        y = y.view(len(y) // num_samples, num_samples)\n",
    "        assert (y[:, 0] == y[:, -1]).all()\n",
    "        s1 = y[:, 0].tolist()\n",
    "        assert len(set(s1)) == len(s1)\n",
    "\n",
    "        x = x.cuda(non_blocking=True)\n",
    "        d = model(x)\n",
    "        d = d.view(len(x) // num_samples, num_samples, emb)\n",
    "        loss = 0\n",
    "        for i in range(num_samples):\n",
    "            for j in range(num_samples):\n",
    "                if i != j:\n",
    "                    l, st = loss_f(d[:, i], d[:, j])\n",
    "                    loss += l\n",
    "                    stats_ep.append({**st, \"loss\": l.item()})\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 10)\n",
    "        optimizer.step()\n",
    "        \n",
    "    scheduler.step()        \n",
    "    rh= evaluate(get_emb_f, ds, hyp_c = hyp_c)\n",
    "    stats_ep = {k: np.mean([x[k] for x in stats_ep]) for k in stats_ep[0]}\n",
    "    stats_ep = {\"recall\": rh, **stats_ep}\n",
    "    if rh > best_recall :\n",
    "        best_recall = rh\n",
    "        best_epoch = epoch\n",
    "    print(\"epoch:\",epoch,\"recall: \", rh)\n",
    "    print(\"best epoch:\",best_epoch,\"best recall: \", best_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae30a67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ab2287",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

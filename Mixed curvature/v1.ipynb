{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57aea838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from functools import partial\n",
    "\n",
    "import net\n",
    "from hyptorch.pmath import dist_matrix\n",
    "from hyptorch import pmath\n",
    "from proxy_anchor import dataset\n",
    "from proxy_anchor.utils import calc_recall_at_k\n",
    "from sampler import UniqueClassSempler\n",
    "from proxy_anchor.dataset import CUBirds, SOP, Cars\n",
    "from proxy_anchor.dataset.Inshop import Inshop_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5a98f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e0c100f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--warm'], dest='warm', nargs=None, const=None, default=1, type=<class 'int'>, choices=None, help='Warmup training epochs', metavar=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch Training')\n",
    "\n",
    "parser.add_argument('--LOG_DIR', \n",
    "    default='/data/xuyunhao/Mixed curvature',\n",
    "    help = 'Path to log folder'\n",
    ")\n",
    "parser.add_argument('--path', default='/data/xuyunhao/datasets', type=str,\n",
    "                    help='path to datasets')\n",
    "parser.add_argument('--ds', default='CUB', type=str,\n",
    "                    help='')\n",
    "parser.add_argument('--num_samples', default=2, type=int,\n",
    "                    help='how many samples per each category in batch')\n",
    "parser.add_argument('--bs', default=200, type=int,\n",
    "                    help='batch size per GPU, e.g. --num_samples 3 --bs 900 means each iteration we sample 300 categories with 3 samples')\n",
    "parser.add_argument('--lr', default=1e-5, type=float,\n",
    "                    help='learning rate')\n",
    "parser.add_argument('--t', default=0.2, type=float,\n",
    "                    help='cross-entropy temperature')\n",
    "parser.add_argument('--emb', default=128, type=int,\n",
    "                    help='output embedding size')\n",
    "parser.add_argument('--freeze', default=0, type=int,\n",
    "                    help='number of blocks in transformer to freeze, None - freeze nothing, 0 - freeze only patch_embed')\n",
    "parser.add_argument('--ep', default=100, type=int,\n",
    "                    help='number of epochs')\n",
    "parser.add_argument('--hyp_c', default=0.1, type=float,\n",
    "                    help='hyperbolic c, \"0\" enables sphere mode')\n",
    "parser.add_argument('--model', default='resnet18', type=str,\n",
    "                    help='model name from timm or torch.hub, i.e. deit_small_distilled_patch16_224, vit_small_patch16_224, dino_vits16')\n",
    "parser.add_argument('--save_emb', default=False, type=bool,\n",
    "                    help='save embeddings of the dataset after training')\n",
    "parser.add_argument('--emb_name', default='emb', type=str,\n",
    "                    help='filename for embeddings')\n",
    "parser.add_argument('--clip_r', default=2.3, type=float,\n",
    "                    help='')\n",
    "parser.add_argument('--resize', default=224, type=int,\n",
    "                    help='image resize')\n",
    "parser.add_argument('--crop', default=224, type=int,\n",
    "                    help='center crop after resize')\n",
    "parser.add_argument('--local_rank', default=0, type=int,\n",
    "                    help='set automatically for distributed training')\n",
    "parser.add_argument('--workers', default = 4, type = int,\n",
    "    dest = 'nb_workers',\n",
    "    help = 'Number of workers for dataloader.'\n",
    ")\n",
    "parser.add_argument('--optimizer', default = 'adamw',\n",
    "    help = 'Optimizer setting'\n",
    ")\n",
    "parser.add_argument('--gpu-id', default = 4, type = int,\n",
    "    help = 'ID of GPU that is used for training.'\n",
    ")\n",
    "parser.add_argument('--bn-freeze', default = 1, type = int,\n",
    "    help = 'Batch normalization parameter freeze'\n",
    ")\n",
    "parser.add_argument('--l2-norm', default = 1, type = int,\n",
    "    help = 'L2 normlization'\n",
    ")\n",
    "parser.add_argument('--lr-decay-step', default = 10, type =int,\n",
    "    help = 'Learning decay step setting'\n",
    ")\n",
    "parser.add_argument('--lr-decay-gamma', default = 0.5, type =float,\n",
    "    help = 'Learning decay gamma setting'\n",
    ")\n",
    "parser.add_argument('--warm', default = 1, type = int,\n",
    "    help = 'Warmup training epochs'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c3f95d",
   "metadata": {},
   "source": [
    "# 投影空间"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3345164b",
   "metadata": {},
   "source": [
    "## 双曲面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6425327c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hexp0(u, *, c=1.0):\n",
    "    c = torch.as_tensor(c).type_as(u)\n",
    "    return _hexpmap0(u, c)\n",
    "\n",
    "def _hexpmap0(u, c):\n",
    "    sqrt_c = c ** 0.5\n",
    "    u_norm = torch.clamp_min(u.norm(dim=-1, p=2, keepdim=True), 1e-5)\n",
    "    gamm_1 = torch.cosh(sqrt_c*u_norm)/sqrt_c\n",
    "    gamm_2 = torch.sinh(sqrt_c*u_norm)*u/(sqrt_c * u_norm)\n",
    "    gamma = torch.cat([gamm_1, gamm_2],dim = 1)\n",
    "    return gamma\n",
    "    \n",
    "def _dist_matrix_h(x, y, c):\n",
    "    sqrt_c = c ** 0.5\n",
    "    b = torch.ones_like(x).cuda()\n",
    "    b[:, 0] = b[:, 0] * (-1)\n",
    "    x2 = x * b\n",
    "    xy_l = torch.einsum(\"ij,kj->ik\", (x2, y))\n",
    "    return (1/sqrt_c * pmath.arcosh(c*xy_l))\n",
    "\n",
    "\n",
    "def dist_matrix_h(x, y, c=1.0):\n",
    "    c = torch.as_tensor(c).type_as(x)\n",
    "    return _dist_matrix_h(x, y, c)\n",
    "\n",
    "class ToHyperbolic(nn.Module):\n",
    "    def __init__(self, c, clip_r=None):\n",
    "        super(ToHyperbolic, self).__init__()\n",
    "        self.register_parameter(\"xp\", None)\n",
    "        self.c = c\n",
    "        self.clip_r = clip_r\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.clip_r is not None:\n",
    "            x_norm = torch.norm(x, dim=-1, keepdim=True) + 1e-5\n",
    "            fac =  torch.minimum(\n",
    "                torch.ones_like(x_norm), \n",
    "                self.clip_r / x_norm\n",
    "            )\n",
    "            x = x * fac\n",
    "        return pmath.project(hexp0(x, c=self.c), c=self.c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c991d3e4",
   "metadata": {},
   "source": [
    "## 超球面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c6e72c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sexp0(u, *, c=1.0):\n",
    "    c = torch.as_tensor(c).type_as(u)\n",
    "    return _sexpmap0(u, c)\n",
    "\n",
    "def _sexpmap0(u, c):\n",
    "    sqrt_c = c ** 0.5\n",
    "    u_norm = torch.clamp_min(u.norm(dim=-1, p=2, keepdim=True), 1e-5)\n",
    "    gamm_1 = torch.cos(sqrt_c*u_norm)/sqrt_c\n",
    "    gamm_2 = torch.sin(sqrt_c*u_norm)*u/(sqrt_c * u_norm)\n",
    "    gamma = torch.cat([gamm_1, gamm_2],dim = 1)\n",
    "    return gamma\n",
    "\n",
    "def _dist_matrix_s(x, y, c):\n",
    "    sqrt_c = c ** 0.5\n",
    "    xy_l = torch.einsum(\"ij,kj->ik\", (x, y))\n",
    "    return (1/sqrt_c * torch.acos(c*xy_l))\n",
    "\n",
    "\n",
    "def dist_matrix_s(x, y, c=1.0):\n",
    "    c = torch.as_tensor(c).type_as(x)\n",
    "    return _dist_matrix_s(x, y, c)\n",
    "\n",
    "class ToHypersphere(nn.Module):\n",
    "    def __init__(self, c, clip_r=None):\n",
    "        super(ToHypersphere, self).__init__()\n",
    "        self.register_parameter(\"xp\", None)\n",
    "        self.c = c\n",
    "        self.clip_r = clip_r\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.clip_r is not None:\n",
    "            x_norm = torch.norm(x, dim=-1, keepdim=True) + 1e-5\n",
    "            fac =  torch.minimum(\n",
    "                torch.ones_like(x_norm), \n",
    "                self.clip_r / x_norm\n",
    "            )\n",
    "            x = x * fac\n",
    "        return pmath.project(sexp0(x, c=self.c), c=self.c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c736bca5",
   "metadata": {},
   "source": [
    "## 投影超球"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ab7ea72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dexp0(u, *, c=1.0):\n",
    "    c = torch.as_tensor(c).type_as(u)\n",
    "    return _dexp0(u, c)\n",
    "\n",
    "\n",
    "def _dexp0(u, c):\n",
    "    sqrt_c = c ** 0.5\n",
    "    u_norm = torch.clamp_min(u.norm(dim=-1, p=2, keepdim=True), 1e-5)\n",
    "    gamma_1 = torch.tan(sqrt_c * u_norm) * u / (sqrt_c * u_norm)\n",
    "    return gamma_1\n",
    "\n",
    "def _dist_matrix_d(x, y, c):\n",
    "    xy =torch.einsum(\"ij,kj->ik\", (x, y))  # B x C\n",
    "    x2 = x.pow(2).sum(-1, keepdim=True)  # B x 1\n",
    "    y2 = y.pow(2).sum(-1, keepdim=True)  # C x 1\n",
    "    sqrt_c = c ** 0.5\n",
    "    num1 = 2*c*(x2+y2.permute(1, 0)-2*xy)\n",
    "    num2 = torch.mul((1+c*x2),(1+c*y2.permute(1, 0)))\n",
    "    return (1/sqrt_c * torch.acos(1-num1/num2))\n",
    "\n",
    "\n",
    "def dist_matrix_d(x, y, c=1.0):\n",
    "    c = torch.as_tensor(c).type_as(x)\n",
    "    return _dist_matrix_d(x, y, c)\n",
    "\n",
    "class ToProjection_hypersphere(nn.Module):\n",
    "    def __init__(self, c, clip_r=None):\n",
    "        super(ToProjection_hypersphere, self).__init__()\n",
    "        self.register_parameter(\"xp\", None)\n",
    "        self.c = c\n",
    "        self.clip_r = clip_r\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.clip_r is not None:\n",
    "            x_norm = torch.norm(x, dim=-1, keepdim=True) + 1e-5\n",
    "            fac =  torch.minimum(\n",
    "                torch.ones_like(x_norm), \n",
    "                self.clip_r / x_norm\n",
    "            )\n",
    "            x = x * fac\n",
    "        return pmath.project(dexp0(x, c=self.c), c=self.c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ee6e9f",
   "metadata": {},
   "source": [
    "## 计算权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af0f1a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class weight_conculate(nn.Module):\n",
    "    def __init__(self, sigma, topk):\n",
    "        super(weight_conculate, self).__init__()\n",
    "        self.sigma = sigma\n",
    "        self.topk = topk\n",
    "\n",
    "    def forward(self, dist_matrix):\n",
    "\n",
    "        N = len(dist_matrix)        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            W_P = torch.exp(-dist_matrix.pow(2) / self.sigma)\n",
    "\n",
    "            topk_index = torch.topk(W_P, self.topk)[1]\n",
    "            topk_half_index = topk_index[:, :int(np.around(self.topk/2))]\n",
    "\n",
    "            W_NN = torch.zeros_like(W_P).scatter_(1, topk_index, torch.ones_like(W_P))\n",
    "            V = ((W_NN + W_NN.t())/2 == 1).float()\n",
    "\n",
    "            W_C_tilda = torch.zeros_like(W_P)\n",
    "            for i in range(N):\n",
    "                indNonzero = torch.where(V[i, :]!=0)[0]\n",
    "                W_C_tilda[i, indNonzero] = (V[:,indNonzero].sum(1) / len(indNonzero))[indNonzero]\n",
    "                \n",
    "            W_C_hat = W_C_tilda[topk_half_index].mean(1)\n",
    "            W_C = (W_C_hat + W_C_hat.t())/2\n",
    "            W = (W_P + W_C)/2\n",
    "\n",
    "        return W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab7d2d3",
   "metadata": {},
   "source": [
    "# 损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a31a4afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(e0, e1, p0, p1, h0, h1, d0, d1, s0, s1, tau, hyp_c, weightconculate):\n",
    "    # x0 and x1 - positive pair\n",
    "    # tau - temperature\n",
    "    # hyp_c - hyperbolic curvature, \"0\" enables sphere mode\n",
    "    dist_e = lambda x, y: x @ y.t()\n",
    "    dist_p = lambda x, y: -dist_matrix(x, y, c=hyp_c)\n",
    "    dist_h = lambda x, y: -dist_matrix_h(x, y, c=hyp_c)\n",
    "    dist_d = lambda x, y: -dist_matrix_d(x, y, c=hyp_c)\n",
    "    dist_s = lambda x, y: -dist_matrix_s(x, y, c=hyp_c)\n",
    "    \n",
    "    dist_e0 = dist_e(e0, e0)\n",
    "    dist_p0 = dist_p(p0, p0)\n",
    "    dist_h0 = dist_h(h0, h0)\n",
    "    dist_d0 = dist_d(d0, d0)\n",
    "    dist_s0 = dist_s(s0, s0)\n",
    "    w_e0 = weightconculate(-dist_e0).cuda()\n",
    "    w_p0 = weightconculate(-dist_p0).cuda()\n",
    "    w_h0 = weightconculate(-dist_h0).cuda()\n",
    "    w_d0 = weightconculate(-dist_d0).cuda()\n",
    "    w_s0 = weightconculate(-dist_s0).cuda()\n",
    "    \n",
    "    dist_e1 = dist_e(e0, e1)\n",
    "    dist_p1 = dist_p(p0, p1)\n",
    "    dist_h1 = dist_h(h0, h1)\n",
    "    dist_d1 = dist_d(d0, d1)\n",
    "    dist_s1 = dist_s(s0, s1)\n",
    "    w_e1 = weightconculate(-dist_e1).cuda()\n",
    "    w_p1 = weightconculate(-dist_p1).cuda()\n",
    "    w_h1 = weightconculate(-dist_h1).cuda()\n",
    "    w_d1 = weightconculate(-dist_d1).cuda()\n",
    "    w_s1 = weightconculate(-dist_s1).cuda()\n",
    "    \n",
    "    bsize = e0.shape[0]\n",
    "    target = torch.arange(bsize).cuda()\n",
    "    eye_mask = torch.eye(bsize).cuda() * 1e9\n",
    "    logits00 = (w_e0*dist_e0+w_p0*dist_p0+w_h0*dist_h0+w_d0*dist_d0+w_s0*dist_s0) / tau - eye_mask\n",
    "    logits01 = (w_e1*dist_e1+w_p1*dist_p1+w_h1*dist_h1+w_d1*dist_d1+w_s1*dist_s1) / tau\n",
    "    logits = torch.cat([logits01, logits00], dim=1)\n",
    "    logits -= logits.max(1, keepdim=True)[0].detach()\n",
    "    loss = F.cross_entropy(logits, target)\n",
    "    stats = {\n",
    "        \"logits/min\": logits01.min().item(),\n",
    "        \"logits/mean\": logits01.mean().item(),\n",
    "        \"logits/max\": logits01.max().item(),\n",
    "        \"logits/acc\": (logits01.argmax(-1) == target).float().mean().item(),\n",
    "    }\n",
    "    return loss, stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde5327b",
   "metadata": {},
   "source": [
    "# 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "617e34b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.nn.init as init\n",
    "import hyptorch.nn as hypnn\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "class Resnet18(nn.Module):\n",
    "    def __init__(self,embedding_size, pretrained=True, bn_freeze = True, hyp_c = 0, clip_r = 0):\n",
    "        super(Resnet18, self).__init__()\n",
    "\n",
    "        self.model = resnet18(pretrained)\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_ftrs = self.model.fc.in_features\n",
    "        self.model.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.model.gmp = nn.AdaptiveMaxPool2d(1)\n",
    "        \n",
    "        self.Elayer = NormLayer()\n",
    "        self.Player = hypnn.ToPoincare(\n",
    "            c=hyp_c,\n",
    "            ball_dim=embedding_size,\n",
    "            riemannian=False,\n",
    "            clip_r=clip_r,\n",
    "        )\n",
    "        self.Hlayer = ToHyperbolic(c=hyp_c, clip_r=clip_r)\n",
    "        self.Dlayer = ToProjection_hypersphere(c=hyp_c, clip_r=clip_r)\n",
    "        self.Slayer = ToHypersphere(c=hyp_c, clip_r=clip_r)\n",
    "\n",
    "        self.model.embedding = nn.Sequential(nn.Linear(self.num_ftrs, self.embedding_size), self.Elayer)\n",
    "        self.model.embeddingP = nn.Sequential(nn.Linear(self.num_ftrs, self.embedding_size), self.Player)\n",
    "        self.model.embeddingH = nn.Sequential(nn.Linear(self.num_ftrs, self.embedding_size), self.Hlayer)\n",
    "        self.model.embeddingD = nn.Sequential(nn.Linear(self.num_ftrs, self.embedding_size), self.Dlayer)\n",
    "        self.model.embeddingS = nn.Sequential(nn.Linear(self.num_ftrs, self.embedding_size), self.Slayer)\n",
    "        \n",
    "        self._initialize_weights()\n",
    "\n",
    "        if bn_freeze:\n",
    "            for m in self.model.modules():\n",
    "                if isinstance(m, nn.BatchNorm2d):\n",
    "                    m.eval()\n",
    "                    m.weight.requires_grad_(False)\n",
    "                    m.bias.requires_grad_(False)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model.conv1(x)\n",
    "        x = self.model.bn1(x)\n",
    "        x = self.model.relu(x)\n",
    "        x = self.model.maxpool(x)\n",
    "        x = self.model.layer1(x)\n",
    "        x = self.model.layer2(x)\n",
    "        x = self.model.layer3(x)\n",
    "        x = self.model.layer4(x)\n",
    "\n",
    "        avg_x = self.model.gap(x)\n",
    "        max_x = self.model.gmp(x)\n",
    "\n",
    "        x = max_x + avg_x\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x_e = self.model.embedding(x)\n",
    "        x_p = self.model.embeddingP(x)\n",
    "        x_h = self.model.embeddingH(x)\n",
    "        x_d = self.model.embeddingD(x)\n",
    "        x_s = self.model.embeddingS(x)\n",
    "        return x_e, x_p, x_h, x_d, x_s\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        init.kaiming_normal_(self.model.embedding[0].weight, mode='fan_out')\n",
    "        init.constant_(self.model.embedding[0].bias, 0)\n",
    "        init.kaiming_normal_(self.model.embeddingP[0].weight, mode='fan_out')\n",
    "        init.constant_(self.model.embeddingP[0].bias, 0)\n",
    "        init.kaiming_normal_(self.model.embeddingH[0].weight, mode='fan_out')\n",
    "        init.constant_(self.model.embeddingH[0].bias, 0)\n",
    "        init.kaiming_normal_(self.model.embeddingD[0].weight, mode='fan_out')\n",
    "        init.constant_(self.model.embeddingD[0].bias, 0)\n",
    "        init.kaiming_normal_(self.model.embeddingS[0].weight, mode='fan_out')\n",
    "        init.constant_(self.model.embeddingS[0].bias, 0)\n",
    "\n",
    "class NormLayer(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return F.normalize(x, p=2, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abbf422",
   "metadata": {},
   "source": [
    "# 验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4a36724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(get_emb_f, ds_name, hyp_c):\n",
    "    if ds_name != \"Inshop\":\n",
    "        emb_head = get_emb_f(ds_type=\"eval\")\n",
    "        recall_head = get_recall(*emb_head, ds_name, hyp_c)\n",
    "    else:\n",
    "        emb_head_query = get_emb_f(ds_type=\"query\")\n",
    "        emb_head_gal = get_emb_f(ds_type=\"gallery\")\n",
    "        emb_body_query = get_emb_f(ds_type=\"query\", skip_head=True)\n",
    "        emb_body_gal = get_emb_f(ds_type=\"gallery\", skip_head=True)\n",
    "        recall_head = get_recall_inshop(*emb_head_query, *emb_head_gal, hyp_c)\n",
    "        recall_body = get_recall_inshop(*emb_body_query, *emb_body_gal, 0)\n",
    "    return recall_head\n",
    "\n",
    "def get_recall(e, p, h, d, s, y, ds_name, hyp_c):\n",
    "    if ds_name == \"CUB\" or ds_name == \"Cars\":\n",
    "        k_list = [1, 2, 4, 8, 16, 32]\n",
    "    elif ds_name == \"SOP\":\n",
    "        k_list = [1, 10, 100, 1000]\n",
    "\n",
    "    dist_m = torch.empty(len(e), len(e), device=\"cuda\")\n",
    "    for i in range(len(x)):\n",
    "        dist_m[i : i + 1] = -dist_matrix(p[i : i + 1], p, hyp_c)-dist_matrix_h(h[i : i + 1], h, hyp_c)-dist_matrix_d(d[i : i + 1], d, hyp_c)-dist_matrix_s(s[i : i + 1], s, hyp_c)\n",
    "    dist_m = dist_m + e @ e.t()\n",
    "\n",
    "    y_cur = y[dist_m.topk(1 + max(k_list), largest=True)[1][:, 1:]]\n",
    "    y = y.cpu()\n",
    "    y_cur = y_cur.float().cpu()\n",
    "    recall = [calc_recall_at_k(y, y_cur, k) for k in k_list]\n",
    "    print(recall)\n",
    "    return recall[0]\n",
    "\n",
    "def get_emb(\n",
    "    model,\n",
    "    ds,\n",
    "    path,\n",
    "    ds_type=\"eval\",\n",
    "    world_size=1,\n",
    "    num_workers=8,\n",
    "):\n",
    "    eval_tr = dataset.utils.make_transform(\n",
    "        is_train = True, \n",
    "        is_inception = (args.model == 'bn_inception')\n",
    "    )\n",
    "    ds_eval = ds(path, ds_type, eval_tr)\n",
    "    if world_size == 1:\n",
    "        sampler = None\n",
    "    else:\n",
    "        sampler = torch.utils.data.distributed.DistributedSampler(ds_eval)\n",
    "    dl_eval = DataLoader(\n",
    "        dataset=ds_eval,\n",
    "        batch_size=100,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        sampler=sampler,\n",
    "    )\n",
    "    model.eval()\n",
    "    e, p, h, d, s, y = eval_dataset(model, dl_eval)\n",
    "    y = y.cuda()\n",
    "    model.train()\n",
    "    return e, p, h, d, s, y\n",
    "\n",
    "def eval_dataset(model, dl):\n",
    "    all_xe, all_xp, all_xh, all_xd, all_xs, all_y = [], [], [], [], [], []\n",
    "    for x, y in dl:\n",
    "        with torch.no_grad():\n",
    "            x = x.cuda(non_blocking=True)\n",
    "            e, p, h, d, s = model(x)\n",
    "            all_xe.append(e)\n",
    "            all_xp.append(p)\n",
    "            all_xh.append(h)\n",
    "            all_xs.append(d)\n",
    "            all_xd.append(s)\n",
    "        all_y.append(y)\n",
    "    return torch.cat(all_xe), torch.cat(all_xp), torch.cat(all_xh), torch.cat(all_xd), torch.cat(all_xs), torch.cat(all_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ac20fc",
   "metadata": {},
   "source": [
    "# 主函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24a49007",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training parameters: {'LOG_DIR': '/data/xuyunhao/Mixed curvature', 'path': '/data/xuyunhao/datasets', 'ds': 'CUB', 'num_samples': 2, 'bs': 200, 'lr': 1e-05, 't': 0.2, 'emb': 128, 'freeze': 0, 'ep': 100, 'hyp_c': 0.1, 'eval_ep': '[100]', 'model': 'resnet18', 'save_emb': False, 'emb_name': 'emb', 'clip_r': 2.3, 'resize': 224, 'crop': 224, 'local_rank': 0, 'nb_workers': 4, 'optimizer': 'adamw', 'gpu_id': 4, 'bn_freeze': 1, 'l2_norm': 1, 'lr_decay_step': 10, 'lr_decay_gamma': 0.5, 'warm': 1}\n",
      "Training for 100 epochs.\n",
      "number:  0\n",
      "[0.008440243079000676, 0.008440243079000676, 0.008440243079000676, 0.008440243079000676, 0.008440243079000676, 0.008440243079000676]\n",
      "epoch: 0 recall:  0.008440243079000676\n",
      "best epoch: 0 best recall:  0.008440243079000676\n",
      "number:  0\n",
      "[0.008440243079000676, 0.008440243079000676, 0.008440243079000676, 0.008440243079000676, 0.008440243079000676, 0.008440243079000676]\n",
      "epoch: 1 recall:  0.008440243079000676\n",
      "best epoch: 0 best recall:  0.008440243079000676\n",
      "number:  0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3348478/1052909231.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0mrh\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_emb_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyp_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_rank\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3348478/1437319275.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(get_emb_f, ds_name, hyp_c)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_emb_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyp_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mds_name\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"Inshop\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0memb_head\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_emb_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"eval\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mrecall_head\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_recall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0memb_head\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyp_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3348478/1437319275.py\u001b[0m in \u001b[0;36mget_emb\u001b[0;34m(model, ds, path, ds_type, world_size, num_workers)\u001b[0m\n\u001b[1;32m     57\u001b[0m     )\n\u001b[1;32m     58\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3348478/1437319275.py\u001b[0m in \u001b[0;36meval_dataset\u001b[0;34m(model, dl)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0meval_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mall_xe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_xp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_xh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_xd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1207\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1208\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1209\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1161\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1163\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1164\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1009\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1011\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1012\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.7/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "args,_ = parser.parse_known_args()\n",
    "if args.gpu_id != -1:\n",
    "    torch.cuda.set_device(args.gpu_id)\n",
    "LOG_DIR = args.LOG_DIR + '/logs_{}/{}_embedding{}_{}_lr{}_batch{}'.format(args.ds, args.model, args.emb, args.optimizer, args.lr, args.bs)\n",
    "world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "\n",
    "train_tr = dataset.utils.make_transform(\n",
    "    is_train = True, \n",
    "    is_inception = (args.model == 'bn_inception')\n",
    ")\n",
    "\n",
    "ds_list = {\"CUB\": CUBirds, \"SOP\": SOP, \"Cars\": Cars, \"Inshop\": Inshop_Dataset}\n",
    "ds_class = ds_list[args.ds]\n",
    "ds_train = ds_class(args.path, \"train\", train_tr)\n",
    "\n",
    "sampler = UniqueClassSempler(\n",
    "    ds_train.ys, args.num_samples, args.local_rank, world_size\n",
    ")\n",
    "dl_train = DataLoader(\n",
    "    dataset=ds_train,\n",
    "    sampler=sampler,\n",
    "    batch_size=args.bs,\n",
    "    num_workers=args.nb_workers,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "if args.model.find('resnet18')+1:\n",
    "    model = Resnet18(embedding_size=args.emb, pretrained=True, bn_freeze = args.bn_freeze, hyp_c =args.hyp_c, clip_r = args.clip_r).cuda().train()\n",
    "if args.gpu_id != -1:\n",
    "    unfreeze_model_param = list(model.model.embedding.parameters())+list(model.model.embeddingP.parameters())+list(model.model.embeddingH.parameters())+list(model.model.embeddingD.parameters())+list(model.model.embeddingS.parameters())\n",
    "    for param in list(set(model.parameters()).difference(set(unfreeze_model_param))):\n",
    "        param.requires_grad = False \n",
    "else:\n",
    "    unfreeze_model_param = list(model.module.model.embedding.parameters())+list(model.module.model.embeddingP.parameters())+list(model.module.model.embeddingH.parameters())+list(model.module.model.embeddingD.parameters())+list(model.module.model.embeddingS.parameters())\n",
    "    for param in list(set(model.module.parameters()).difference(set(unfreeze_model_param))):\n",
    "        param.requires_grad = False    \n",
    "    \n",
    "if args.gpu_id == -1:\n",
    "    model = nn.DataParallel(model).cuda().train()\n",
    "wt = weight_conculate(sigma = 1, topk = 9)\n",
    "loss_f = partial(contrastive_loss, tau=args.t, hyp_c=args.hyp_c, weightconculate= wt)\n",
    "\n",
    "get_emb_f = partial(\n",
    "    get_emb,\n",
    "    model=model,\n",
    "    ds=ds_class,\n",
    "    path=args.path,\n",
    "    num_workers=args.nb_workers,\n",
    "    world_size=world_size,\n",
    ")\n",
    "\n",
    "# Train Parameters\n",
    "param_groups = [\n",
    "    {'params': list(set(model.parameters()).difference(set(unfreeze_model_param))) if args.gpu_id != -1 else \n",
    "                 list(set(model.module.parameters()).difference(set(unfreeze_model_param)))},\n",
    "    {'params': unfreeze_model_param, 'lr':float(args.lr) * 1},\n",
    "]\n",
    "\n",
    "optimizer = optim.AdamW(param_groups, lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=args.lr_decay_step, gamma = args.lr_decay_gamma)\n",
    "print(\"Training parameters: {}\".format(vars(args)))\n",
    "print(\"Training for {} epochs.\".format(args.ep))\n",
    "losses_list = []\n",
    "best_recall= 0\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(0, args.ep):\n",
    "    model.train()\n",
    "    bn_freeze = args.bn_freeze\n",
    "    if bn_freeze:\n",
    "        modules = model.model.modules() if args.gpu_id != -1 else model.module.model.modules()\n",
    "        for m in modules: \n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                m.eval()\n",
    "\n",
    "    losses_per_epoch = []\n",
    "    \n",
    "     #Warmup: Train only new params, helps stabilize learning.\n",
    "#    if args.warm > 0:\n",
    "#        if args.gpu_id != -1:\n",
    "#            unfreeze_model_param = list(model.model.embedding.parameters())\n",
    "#        else:\n",
    "#            unfreeze_model_param = list(model.module.model.embedding.parameters())\n",
    "\n",
    "#        if epoch == 0:\n",
    "#            for param in list(set(model.parameters()).difference(set(unfreeze_model_param))):\n",
    "#                param.requires_grad = False\n",
    "#        if epoch == args.warm:\n",
    "#            for param in list(set(model.parameters()).difference(set(unfreeze_model_param))):\n",
    "#                param.requires_grad = True\n",
    "\n",
    "    sampler.set_epoch(epoch)\n",
    "    stats_ep = []\n",
    "    for x, y in dl_train:\n",
    "        y = y.view(len(y) // args.num_samples, args.num_samples)\n",
    "        assert (y[:, 0] == y[:, -1]).all()\n",
    "        s1 = y[:, 0].tolist()\n",
    "        assert len(set(s1)) == len(s1)\n",
    "\n",
    "        x = x.cuda(non_blocking=True)\n",
    "        e, p, h, d, s = model(x)\n",
    "        e = e.view(len(x) // args.num_samples, args.num_samples, args.emb)\n",
    "        p = p.view(len(x) // args.num_samples, args.num_samples, args.emb)\n",
    "        h = h.view(len(x) // args.num_samples, args.num_samples, args.emb+1)\n",
    "        d = d.view(len(x) // args.num_samples, args.num_samples, args.emb)\n",
    "        s = s.view(len(x) // args.num_samples, args.num_samples, args.emb+1)\n",
    "        loss = 0\n",
    "        for i in range(args.num_samples):\n",
    "            for j in range(args.num_samples):\n",
    "                if i != j:\n",
    "                    l, st = loss_f(e[:, i], e[:, j], p[:, i], p[:, j], h[:, i], h[:, j],d[:, i], d[:, j],s[:, i], s[:, j])\n",
    "                    loss += l\n",
    "                    stats_ep.append({**st, \"loss\": l.item()})\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 10)\n",
    "        optimizer.step()\n",
    "            \n",
    "    scheduler.step()\n",
    "    rh= evaluate(get_emb_f, args.ds, args.hyp_c)\n",
    "        \n",
    "    if args.local_rank == 0:\n",
    "        stats_ep = {k: np.mean([x[k] for x in stats_ep]) for k in stats_ep[0]}\n",
    "        stats_ep = {\"recall\": rh, **stats_ep}\n",
    "        if rh > best_recall :\n",
    "            best_recall = rh\n",
    "            best_epoch = epoch\n",
    "        print(\"epoch:\",epoch,\"recall: \", rh)\n",
    "        print(\"best epoch:\",best_epoch,\"best recall: \", best_recall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

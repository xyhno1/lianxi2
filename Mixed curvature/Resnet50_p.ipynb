{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8e850fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from functools import partial\n",
    "\n",
    "import net\n",
    "from hyptorch.pmath import dist_matrix\n",
    "from proxy_anchor import dataset\n",
    "from proxy_anchor.utils import calc_recall_at_k\n",
    "from sampler import UniqueClassSempler\n",
    "from proxy_anchor.dataset import CUBirds, SOP, Cars\n",
    "from proxy_anchor.dataset.Inshop import Inshop_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4743d016",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4,5'\n",
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0767f78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/data/xuyunhao/datasets'\n",
    "ds = 'CUB'\n",
    "num_samples = 2\n",
    "bs = 200\n",
    "lr = 1e-5\n",
    "t = 0.2\n",
    "emb = 384\n",
    "ep = 100\n",
    "local_rank = 0\n",
    "workers = 8\n",
    "optimizer = 'adamw'\n",
    "lr_decay_step = 10\n",
    "lr_decay_gamma = 0.5\n",
    "\n",
    "model =  'resnet50'\n",
    "hyp_c = 0.1\n",
    "clip_r  = 2.3\n",
    "resize = 224\n",
    "crop = 224\n",
    "gpu_id = -1\n",
    "bn_freeze = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9b2c03",
   "metadata": {},
   "source": [
    "# 庞加莱圆盘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02188e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tensor_dot(x, y):\n",
    "    res = torch.einsum(\"ij,kj->ik\", (x, y))\n",
    "    return res\n",
    "\n",
    "class Artanh(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        x = x.clamp(-1 + 1e-5, 1 - 1e-5)\n",
    "        ctx.save_for_backward(x)\n",
    "        res = (torch.log_(1 + x).sub_(torch.log_(1 - x))).mul_(0.5)\n",
    "        return res\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        (input,) = ctx.saved_tensors\n",
    "        return grad_output / (1 - input ** 2)\n",
    "    \n",
    "def artanh(x):\n",
    "    return Artanh.apply(x)\n",
    "\n",
    "def _mobius_addition_batch(x, y, c):\n",
    "    xy = _tensor_dot(x, y)  # B x C\n",
    "    x2 = x.pow(2).sum(-1, keepdim=True)  # B x 1\n",
    "    y2 = y.pow(2).sum(-1, keepdim=True)  # C x 1\n",
    "    num = 1 + 2 * c * xy + c * y2.permute(1, 0)  # B x C\n",
    "    num = num.unsqueeze(2) * x.unsqueeze(1)\n",
    "    num = num + (1 - c * x2).unsqueeze(2) * y  # B x C x D\n",
    "    denom_part1 = 1 + 2 * c * xy  # B x C\n",
    "    denom_part2 = c ** 2 * x2 * y2.permute(1, 0)\n",
    "    denom = denom_part1 + denom_part2\n",
    "    res = num / (denom.unsqueeze(2) + 1e-5)\n",
    "    return res\n",
    "\n",
    "def _dist_matrix(x, y, c):\n",
    "    sqrt_c = c ** 0.5\n",
    "    return (\n",
    "        2\n",
    "        / sqrt_c\n",
    "        * artanh(sqrt_c * torch.norm(_mobius_addition_batch(-x, y, c=c), dim=-1))\n",
    "    )\n",
    "\n",
    "\n",
    "def dist_matrix(x, y, c=1.0):\n",
    "    c = torch.as_tensor(c).type_as(x)\n",
    "    return _dist_matrix(x, y, c)\n",
    "\n",
    "def tanh(x, clamp=15):\n",
    "    return x.clamp(-clamp, clamp).tanh()\n",
    "\n",
    "def expmap0(u, *, c=1.0):\n",
    "    c = torch.as_tensor(c).type_as(u)\n",
    "    return _expmap0(u, c)\n",
    "\n",
    "def _expmap0(u, c):\n",
    "    sqrt_c = c ** 0.5\n",
    "    u_norm = torch.clamp_min(u.norm(dim=-1, p=2, keepdim=True), 1e-5)\n",
    "    gamma_1 = tanh(sqrt_c * u_norm) * u / (sqrt_c * u_norm)\n",
    "    return gamma_1\n",
    "\n",
    "def project(x, *, c=1.0):\n",
    "    c = torch.as_tensor(c).type_as(x)\n",
    "    return _project(x, c)\n",
    "\n",
    "def _project(x, c):\n",
    "    norm = torch.clamp_min(x.norm(dim=-1, keepdim=True, p=2), 1e-5)\n",
    "    maxnorm = (1 - 1e-3) / (c ** 0.5)\n",
    "    cond = norm > maxnorm\n",
    "    projected = x / norm * maxnorm\n",
    "    return torch.where(cond, projected, x)\n",
    "\n",
    "class ToPoincare(nn.Module):\n",
    "    r\"\"\"\n",
    "    Module which maps points in n-dim Euclidean space\n",
    "    to n-dim Poincare ball\n",
    "    Also implements clipping from https://arxiv.org/pdf/2107.11472.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, c, clip_r=None):\n",
    "        super(ToPoincare, self).__init__()\n",
    "        self.register_parameter(\"xp\", None)\n",
    "\n",
    "        self.c = c\n",
    "        \n",
    "        self.clip_r = clip_r\n",
    "        self.grad_fix = lambda x: x\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.clip_r is not None:\n",
    "            x_norm = torch.norm(x, dim=-1, keepdim=True) + 1e-5\n",
    "            fac =  torch.minimum(\n",
    "                torch.ones_like(x_norm), \n",
    "                self.clip_r / x_norm\n",
    "            )\n",
    "            x = x * fac\n",
    "        return self.grad_fix(project(expmap0(x, c=self.c), c=self.c))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae303169",
   "metadata": {},
   "source": [
    "# 损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fd72cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(x0, x1, tau, hyp_c):\n",
    "    # x0 and x1 - positive pair\n",
    "    # tau - temperature\n",
    "    # hyp_c - hyperbolic curvature, \"0\" enables sphere mode\n",
    "\n",
    "    dist_f = lambda x, y: -dist_matrix(x, y, c=hyp_c)\n",
    "    bsize = x0.shape[0]\n",
    "    target = torch.arange(bsize).cuda()\n",
    "    eye_mask = torch.eye(bsize).cuda() * 1e9\n",
    "    logits00 = dist_f(x0, x0) / tau - eye_mask\n",
    "    logits01 = dist_f(x0, x1) / tau\n",
    "    logits = torch.cat([logits01, logits00], dim=1)\n",
    "    logits -= logits.max(1, keepdim=True)[0].detach()\n",
    "    loss = F.cross_entropy(logits, target)\n",
    "    stats = {\n",
    "        \"logits/min\": logits01.min().item(),\n",
    "        \"logits/mean\": logits01.mean().item(),\n",
    "        \"logits/max\": logits01.max().item(),\n",
    "        \"logits/acc\": (logits01.argmax(-1) == target).float().mean().item(),\n",
    "    }\n",
    "    return loss, stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa799add",
   "metadata": {},
   "source": [
    "# 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2cb5d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.nn.init as init\n",
    "import hyptorch.nn as hypnn\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.models import resnet34\n",
    "from torchvision.models import resnet50\n",
    "from torchvision.models import resnet101"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b2bd4f",
   "metadata": {},
   "source": [
    "## Resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c8ad915",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resnet50(nn.Module):\n",
    "    def __init__(self,embedding_size, pretrained=True, bn_freeze = True, hyp_c = 0, clip_r = 0):\n",
    "        super(Resnet50, self).__init__()\n",
    "\n",
    "        self.model = resnet50(pretrained)\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hyp_c = hyp_c\n",
    "        self.clip_r = clip_r\n",
    "        self.num_ftrs = self.model.fc.in_features\n",
    "        self.model.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.model.gmp = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.Player = ToPoincare(\n",
    "            c=self.hyp_c,\n",
    "            clip_r=self.clip_r,\n",
    "        )\n",
    "        self.model.embedding = nn.Sequential(nn.Linear(self.num_ftrs, self.embedding_size), self.Player)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "        if bn_freeze:\n",
    "            for m in self.model.modules():\n",
    "                if isinstance(m, nn.BatchNorm2d):\n",
    "                    m.eval()\n",
    "                    m.weight.requires_grad_(False)\n",
    "                    m.bias.requires_grad_(False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model.conv1(x)\n",
    "        x = self.model.bn1(x)\n",
    "        x = self.model.relu(x)\n",
    "        x = self.model.maxpool(x)\n",
    "        x = self.model.layer1(x)\n",
    "        x = self.model.layer2(x)\n",
    "        x = self.model.layer3(x)\n",
    "        x = self.model.layer4(x)\n",
    "\n",
    "        avg_x = self.model.gap(x)\n",
    "        max_x = self.model.gmp(x)\n",
    "\n",
    "        x = max_x + avg_x\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x_p = self.model.embedding(x)\n",
    "        return x_p\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        init.kaiming_normal_(self.model.embedding[0].weight, mode='fan_out')\n",
    "        init.constant_(self.model.embedding[0].bias, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3465a3",
   "metadata": {},
   "source": [
    "## Resnet101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74faffc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resnet101(nn.Module):\n",
    "    def __init__(self,embedding_size, pretrained=True, bn_freeze = True, hyp_c = 0, clip_r = 0):\n",
    "        super(Resnet101, self).__init__()\n",
    "\n",
    "        self.model = resnet101(pretrained)\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hyp_c = hyp_c\n",
    "        self.clip_r = clip_r\n",
    "        self.num_ftrs = self.model.fc.in_features\n",
    "        self.model.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.model.gmp = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.Player = ToPoincare(\n",
    "            c=self.hyp_c,\n",
    "            clip_r=self.clip_r,\n",
    "        )\n",
    "        self.model.embedding = nn.Sequential(nn.Linear(self.num_ftrs, self.embedding_size), self.Player)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "        if bn_freeze:\n",
    "            for m in self.model.modules():\n",
    "                if isinstance(m, nn.BatchNorm2d):\n",
    "                    m.eval()\n",
    "                    m.weight.requires_grad_(False)\n",
    "                    m.bias.requires_grad_(False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model.conv1(x)\n",
    "        x = self.model.bn1(x)\n",
    "        x = self.model.relu(x)\n",
    "        x = self.model.maxpool(x)\n",
    "        x = self.model.layer1(x)\n",
    "        x = self.model.layer2(x)\n",
    "        x = self.model.layer3(x)\n",
    "        x = self.model.layer4(x)\n",
    "\n",
    "        avg_x = self.model.gap(x)\n",
    "        max_x = self.model.gmp(x)\n",
    "\n",
    "        x = max_x + avg_x\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x_p = self.model.embedding(x)\n",
    "        return x_p\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        init.kaiming_normal_(self.model.embedding[0].weight, mode='fan_out')\n",
    "        init.constant_(self.model.embedding[0].bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "882f9e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(get_emb_f, ds_name, hyp_c):\n",
    "    emb_head = get_emb_f(ds_type=\"eval\")\n",
    "    recall_head = get_recall(*emb_head, ds_name, hyp_c)\n",
    "    return recall_head\n",
    "\n",
    "def get_recall(p, y, ds_name, hyp_c):\n",
    "    if ds_name == \"CUB\" or ds_name == \"Cars\":\n",
    "        k_list = [1, 2, 4, 8, 16, 32]\n",
    "    elif ds_name == \"SOP\":\n",
    "        k_list = [1, 10, 100, 1000]\n",
    "\n",
    "    dist_m = torch.empty(len(p), len(p), device=\"cuda\")\n",
    "    for i in range(len(p)):\n",
    "        dist_m[i : i + 1] = -dist_matrix(p[i : i + 1], p, hyp_c)\n",
    "\n",
    "    y_cur = y[dist_m.topk(1 + max(k_list), largest=True)[1][:, 1:]]\n",
    "    y = y.cpu()\n",
    "    y_cur = y_cur.float().cpu()\n",
    "    recall = [calc_recall_at_k(y, y_cur, k) for k in k_list]\n",
    "    print(recall)\n",
    "    return recall[0]\n",
    "\n",
    "def get_emb(\n",
    "    model,\n",
    "    ds,\n",
    "    path,\n",
    "    ds_type=\"eval\",\n",
    "    world_size=1,\n",
    "    num_workers=8,\n",
    "):\n",
    "    eval_tr = dataset.utils.make_transform(\n",
    "        is_train = True, \n",
    "        is_inception = (model == 'bn_inception')\n",
    "    )\n",
    "    ds_eval = ds(path, ds_type, eval_tr)\n",
    "    if world_size == 1:\n",
    "        sampler = None\n",
    "    else:\n",
    "        sampler = torch.utils.data.distributed.DistributedSampler(ds_eval)\n",
    "    dl_eval = DataLoader(\n",
    "        dataset=ds_eval,\n",
    "        batch_size=100,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        sampler=sampler,\n",
    "    )\n",
    "    model.eval()\n",
    "    p, y = eval_dataset(model, dl_eval)\n",
    "    y = y.cuda()\n",
    "    model.train()\n",
    "    return p, y\n",
    "\n",
    "def eval_dataset(model, dl):\n",
    "    all_xp, all_y = [], []\n",
    "    for x, y in dl:\n",
    "        with torch.no_grad():\n",
    "            x = x.cuda(non_blocking=True)\n",
    "            p= model(x)\n",
    "            all_xp.append(p)\n",
    "        all_y.append(y)\n",
    "    return torch.cat(all_xp), torch.cat(all_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86e838bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 100 epochs.\n",
      "[0.40698852126941254, 0.5379810938555031, 0.6617150573936529, 0.7670492910195814, 0.8583727211343687, 0.9225185685347738]\n",
      "The recall before train:  0.40698852126941254\n",
      "[0.4159351789331533, 0.5418636056718433, 0.6613774476704929, 0.7722822417285617, 0.8597231600270088, 0.9193112761647535]\n",
      "epoch: 0 recall:  0.4159351789331533\n",
      "best epoch: 0 best recall:  0.4159351789331533\n",
      "[0.4299459824442944, 0.5557056043214045, 0.6760634706279541, 0.7835921674544227, 0.8676569885212694, 0.924206617150574]\n",
      "epoch: 1 recall:  0.4299459824442944\n",
      "best epoch: 1 best recall:  0.4299459824442944\n",
      "[0.42943956785955434, 0.5604321404456448, 0.6772451046590142, 0.7780216070222823, 0.8652937204591492, 0.9230249831195139]\n",
      "epoch: 2 recall:  0.42943956785955434\n",
      "best epoch: 1 best recall:  0.4299459824442944\n",
      "[0.437373396353815, 0.5668467251856854, 0.6831532748143146, 0.786799459824443, 0.8664753544902093, 0.924037812288994]\n",
      "epoch: 3 recall:  0.437373396353815\n",
      "best epoch: 3 best recall:  0.437373396353815\n",
      "[0.4471640783254558, 0.5646522619851452, 0.6845037137069547, 0.7866306549628629, 0.863268062120189, 0.9157663740715732]\n",
      "epoch: 4 recall:  0.4471640783254558\n",
      "best epoch: 4 best recall:  0.4471640783254558\n",
      "[0.44817690749493583, 0.5732613099257259, 0.687035786630655, 0.7905131667792032, 0.8686698176907495, 0.9237002025658338]\n",
      "epoch: 5 recall:  0.44817690749493583\n",
      "best epoch: 5 best recall:  0.44817690749493583\n",
      "[0.4589804186360567, 0.5832207967589467, 0.6968264686022958, 0.7949020931802836, 0.8690074274139096, 0.9225185685347738]\n",
      "epoch: 6 recall:  0.4589804186360567\n",
      "best epoch: 6 best recall:  0.4589804186360567\n",
      "[0.44598244429439565, 0.5754557731262661, 0.6927751519243754, 0.7866306549628629, 0.8656313301823092, 0.9181296421336934]\n",
      "epoch: 7 recall:  0.44598244429439565\n",
      "best epoch: 6 best recall:  0.4589804186360567\n",
      "[0.4706279540850776, 0.5945307224848075, 0.7040850776502363, 0.7915259959486833, 0.8639432815665091, 0.9184672518568535]\n",
      "epoch: 8 recall:  0.4706279540850776\n",
      "best epoch: 8 best recall:  0.4706279540850776\n",
      "[0.46573261309925723, 0.5815327481431465, 0.6922687373396353, 0.7925388251181634, 0.8676569885212694, 0.9184672518568535]\n",
      "epoch: 9 recall:  0.46573261309925723\n",
      "best epoch: 8 best recall:  0.4706279540850776\n",
      "[0.46353814989871706, 0.5869345037137069, 0.700877785280216, 0.7901755570560433, 0.8649561107359892, 0.9198176907494936]\n",
      "epoch: 10 recall:  0.46353814989871706\n",
      "best epoch: 8 best recall:  0.4706279540850776\n",
      "[0.4530722484807562, 0.5813639432815665, 0.6922687373396353, 0.7905131667792032, 0.8663065496286293, 0.9181296421336934]\n",
      "epoch: 11 recall:  0.4530722484807562\n",
      "best epoch: 8 best recall:  0.4706279540850776\n",
      "[0.462187711006077, 0.5857528696826468, 0.6941255908170155, 0.7935516542876435, 0.8732275489534098, 0.924375422012154]\n",
      "epoch: 12 recall:  0.462187711006077\n",
      "best epoch: 8 best recall:  0.4706279540850776\n",
      "[0.46910871033085755, 0.5921674544226874, 0.700877785280216, 0.7965901417960837, 0.8700202565833896, 0.9186360567184335]\n",
      "epoch: 13 recall:  0.46910871033085755\n",
      "best epoch: 8 best recall:  0.4706279540850776\n",
      "[0.4665766374071573, 0.5918298446995274, 0.6976704929101958, 0.7954085077650236, 0.862761647535449, 0.9155975692099932]\n",
      "epoch: 14 recall:  0.4665766374071573\n",
      "best epoch: 8 best recall:  0.4706279540850776\n",
      "[0.4532410533423363, 0.587947332883187, 0.7049291019581364, 0.7962525320729237, 0.8717083051991897, 0.9201553004726536]\n",
      "epoch: 15 recall:  0.4532410533423363\n",
      "best epoch: 8 best recall:  0.4706279540850776\n",
      "[0.450371370695476, 0.575962187711006, 0.6912559081701553, 0.7935516542876435, 0.8651249155975692, 0.9189736664415935]\n",
      "epoch: 16 recall:  0.450371370695476\n",
      "best epoch: 8 best recall:  0.4706279540850776\n",
      "[0.4677582714382174, 0.5901417960837272, 0.6976704929101958, 0.7945644834571236, 0.8690074274139096, 0.9225185685347738]\n",
      "epoch: 17 recall:  0.4677582714382174\n",
      "best epoch: 8 best recall:  0.4706279540850776\n",
      "[0.4586428089128967, 0.5830519918973667, 0.6937879810938555, 0.7884875084402431, 0.8605671843349089, 0.9196488858879136]\n",
      "epoch: 18 recall:  0.4586428089128967\n",
      "best epoch: 8 best recall:  0.4706279540850776\n",
      "[0.4687711006076975, 0.5822079675894666, 0.6936191762322755, 0.7895003376097232, 0.8695138419986496, 0.9203241053342336]\n",
      "epoch: 19 recall:  0.4687711006076975\n",
      "best epoch: 8 best recall:  0.4706279540850776\n",
      "[0.46522619851451724, 0.5896353814989872, 0.7037474679270763, 0.7940580688723835, 0.8723835246455098, 0.9211681296421337]\n",
      "epoch: 20 recall:  0.46522619851451724\n",
      "best epoch: 8 best recall:  0.4706279540850776\n",
      "[0.45425388251181636, 0.5860904794058069, 0.6980081026333559, 0.7954085077650236, 0.8695138419986496, 0.9218433490884538]\n",
      "epoch: 21 recall:  0.45425388251181636\n",
      "best epoch: 8 best recall:  0.4706279540850776\n",
      "[0.4611748818365969, 0.5921674544226874, 0.7023970290344362, 0.7962525320729237, 0.8691762322754896, 0.9216745442268738]\n",
      "epoch: 22 recall:  0.4611748818365969\n",
      "best epoch: 8 best recall:  0.4706279540850776\n",
      "[0.46151249155975693, 0.5904794058068873, 0.7015530047265361, 0.7942268737339635, 0.8696826468602296, 0.9198176907494936]\n",
      "epoch: 23 recall:  0.46151249155975693\n",
      "best epoch: 8 best recall:  0.4706279540850776\n",
      "[0.47180958811613777, 0.5914922349763673, 0.7042538825118163, 0.7960837272113437, 0.8652937204591492, 0.9176232275489534]\n",
      "epoch: 24 recall:  0.47180958811613777\n",
      "best epoch: 24 best recall:  0.47180958811613777\n",
      "[0.46826468602295745, 0.587440918298447, 0.7045914922349764, 0.7969277515192438, 0.8685010128291695, 0.9220121539500338]\n",
      "epoch: 25 recall:  0.46826468602295745\n",
      "best epoch: 24 best recall:  0.47180958811613777\n",
      "[0.4736664415935179, 0.5936866981769074, 0.699358541525996, 0.798953409858204, 0.8679945982444295, 0.9231937879810939]\n",
      "epoch: 26 recall:  0.4736664415935179\n",
      "best epoch: 26 best recall:  0.4736664415935179\n",
      "[0.4709655638082377, 0.5941931127616475, 0.701046590141796, 0.7965901417960837, 0.8735651586765699, 0.9215057393652937]\n",
      "epoch: 27 recall:  0.4709655638082377\n",
      "best epoch: 26 best recall:  0.4736664415935179\n",
      "[0.4709655638082377, 0.5855840648210668, 0.6981769074949359, 0.7910195813639432, 0.8637744767049291, 0.9155975692099932]\n",
      "epoch: 28 recall:  0.4709655638082377\n",
      "best epoch: 26 best recall:  0.4736664415935179\n",
      "[0.46826468602295745, 0.5842336259284268, 0.6980081026333559, 0.7908507765023632, 0.8678257933828494, 0.9161039837947332]\n",
      "epoch: 29 recall:  0.46826468602295745\n",
      "best epoch: 26 best recall:  0.4736664415935179\n",
      "[0.4714719783929777, 0.5950371370695476, 0.7018906144496961, 0.7962525320729237, 0.8703578663065497, 0.923869007427414]\n",
      "epoch: 30 recall:  0.4714719783929777\n",
      "best epoch: 26 best recall:  0.4736664415935179\n",
      "[0.46522619851451724, 0.5958811613774476, 0.7113436866981769, 0.8026671168129642, 0.8733963538149899, 0.9223497636731938]\n",
      "epoch: 31 recall:  0.46522619851451724\n",
      "best epoch: 26 best recall:  0.4736664415935179\n",
      "[0.4648885887913572, 0.5930114787305875, 0.6959824442943957, 0.7949020931802836, 0.863099257258609, 0.9142471303173532]\n",
      "epoch: 32 recall:  0.4648885887913572\n",
      "best epoch: 26 best recall:  0.4736664415935179\n",
      "[0.47400405131667794, 0.5886225523295071, 0.7034098582039163, 0.7932140445644834, 0.8659689399054693, 0.9221809588116138]\n",
      "epoch: 33 recall:  0.47400405131667794\n",
      "best epoch: 33 best recall:  0.47400405131667794\n",
      "[0.45965563808237675, 0.5837272113436867, 0.6920999324780553, 0.7916948008102633, 0.8703578663065497, 0.9248818365968939]\n",
      "epoch: 34 recall:  0.45965563808237675\n",
      "best epoch: 33 best recall:  0.47400405131667794\n",
      "[0.46927751519243754, 0.5892977717758271, 0.6953072248480756, 0.7933828494260635, 0.8642808912896691, 0.9176232275489534]\n",
      "epoch: 35 recall:  0.46927751519243754\n",
      "best epoch: 33 best recall:  0.47400405131667794\n",
      "[0.4549291019581364, 0.5817015530047266, 0.700540175557056, 0.8014854827819041, 0.8722147197839297, 0.9230249831195139]\n",
      "epoch: 36 recall:  0.4549291019581364\n",
      "best epoch: 33 best recall:  0.47400405131667794\n",
      "[0.4738352464550979, 0.5957123565158676, 0.7077987846049967, 0.800303848750844, 0.8701890614449697, 0.9176232275489534]\n",
      "epoch: 37 recall:  0.4738352464550979\n",
      "best epoch: 33 best recall:  0.47400405131667794\n",
      "[0.47214719783929776, 0.5914922349763673, 0.7042538825118163, 0.7976029709655638, 0.8688386225523295, 0.9215057393652937]\n",
      "epoch: 38 recall:  0.47214719783929776\n",
      "best epoch: 33 best recall:  0.47400405131667794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.46809588116137746, 0.5909858203916273, 0.6986833220796759, 0.7898379473328832, 0.8593855503038488, 0.9135719108710331]\n",
      "epoch: 39 recall:  0.46809588116137746\n",
      "best epoch: 33 best recall:  0.47400405131667794\n",
      "[0.47012153950033764, 0.5909858203916273, 0.7037474679270763, 0.7972653612424038, 0.8727211343686698, 0.9247130317353139]\n",
      "epoch: 40 recall:  0.47012153950033764\n",
      "best epoch: 33 best recall:  0.47400405131667794\n",
      "[0.475016880486158, 0.5962187711006077, 0.7042538825118163, 0.7918636056718433, 0.8700202565833896, 0.9237002025658338]\n",
      "epoch: 41 recall:  0.475016880486158\n",
      "best epoch: 41 best recall:  0.475016880486158\n",
      "[0.45661715057393654, 0.5823767724510466, 0.6949696151249156, 0.7920324105334233, 0.8693450371370696, 0.9196488858879136]\n",
      "epoch: 42 recall:  0.45661715057393654\n",
      "best epoch: 41 best recall:  0.475016880486158\n",
      "[0.4716407832545577, 0.5952059419311276, 0.7018906144496961, 0.7964213369345037, 0.8671505739365294, 0.9194800810263336]\n",
      "epoch: 43 recall:  0.4716407832545577\n",
      "best epoch: 41 best recall:  0.475016880486158\n",
      "[0.47045914922349763, 0.5928426738690075, 0.7049291019581364, 0.7967589466576638, 0.8717083051991897, 0.9194800810263336]\n",
      "epoch: 44 recall:  0.47045914922349763\n",
      "best epoch: 41 best recall:  0.475016880486158\n",
      "[0.46320054017555706, 0.5869345037137069, 0.700877785280216, 0.7969277515192438, 0.8718771100607697, 0.9245442268737339]\n",
      "epoch: 45 recall:  0.46320054017555706\n",
      "best epoch: 41 best recall:  0.475016880486158\n",
      "[0.47180958811613777, 0.5936866981769074, 0.7052667116812964, 0.8024983119513842, 0.8730587440918298, 0.9235313977042539]\n",
      "epoch: 46 recall:  0.47180958811613777\n",
      "best epoch: 41 best recall:  0.475016880486158\n",
      "[0.46455097906819715, 0.5844024307900068, 0.6944632005401755, 0.7901755570560433, 0.8671505739365294, 0.9182984469952734]\n",
      "epoch: 47 recall:  0.46455097906819715\n",
      "best epoch: 41 best recall:  0.475016880486158\n",
      "[0.4608372721134369, 0.5830519918973667, 0.6973328831870358, 0.7913571910871033, 0.8622552329507089, 0.9161039837947332]\n",
      "epoch: 48 recall:  0.4608372721134369\n",
      "best epoch: 41 best recall:  0.475016880486158\n",
      "[0.4726536124240378, 0.5965563808237677, 0.7076299797434166, 0.7954085077650236, 0.8641120864280891, 0.9198176907494936]\n",
      "epoch: 49 recall:  0.4726536124240378\n",
      "best epoch: 41 best recall:  0.475016880486158\n",
      "[0.4662390276839973, 0.5930114787305875, 0.699189736664416, 0.7920324105334233, 0.8671505739365294, 0.9177920324105334]\n",
      "epoch: 50 recall:  0.4662390276839973\n",
      "best epoch: 41 best recall:  0.475016880486158\n",
      "[0.48193787981093855, 0.5953747467927076, 0.7079675894665767, 0.800135043889264, 0.8728899392302498, 0.9226873733963538]\n",
      "epoch: 51 recall:  0.48193787981093855\n",
      "best epoch: 51 best recall:  0.48193787981093855\n",
      "[0.4618501012829169, 0.5844024307900068, 0.6944632005401755, 0.7911883862255233, 0.8671505739365294, 0.9182984469952734]\n",
      "epoch: 52 recall:  0.4618501012829169\n",
      "best epoch: 51 best recall:  0.48193787981093855\n",
      "[0.46944632005401754, 0.5943619176232275, 0.700877785280216, 0.7952397029034436, 0.8656313301823092, 0.9177920324105334]\n",
      "epoch: 53 recall:  0.46944632005401754\n",
      "best epoch: 51 best recall:  0.48193787981093855\n",
      "[0.46421336934503715, 0.5842336259284268, 0.6971640783254558, 0.7962525320729237, 0.8649561107359892, 0.9149223497636731]\n",
      "epoch: 54 recall:  0.46421336934503715\n",
      "best epoch: 51 best recall:  0.48193787981093855\n",
      "[0.46573261309925723, 0.5980756245779878, 0.7023970290344362, 0.7925388251181634, 0.8674881836596894, 0.9245442268737339]\n",
      "epoch: 55 recall:  0.46573261309925723\n",
      "best epoch: 51 best recall:  0.48193787981093855\n",
      "[0.46708305199189737, 0.5886225523295071, 0.6959824442943957, 0.7886563133018231, 0.8663065496286293, 0.9188048615800135]\n",
      "epoch: 56 recall:  0.46708305199189737\n",
      "best epoch: 51 best recall:  0.48193787981093855\n",
      "[0.4763673193787981, 0.5928426738690075, 0.7056043214044565, 0.7986158001350439, 0.8678257933828494, 0.9211681296421337]\n",
      "epoch: 57 recall:  0.4763673193787981\n",
      "best epoch: 51 best recall:  0.48193787981093855\n",
      "[0.475016880486158, 0.5960499662390277, 0.7047602970965564, 0.800303848750844, 0.8706954760297096, 0.9228561782579339]\n",
      "epoch: 58 recall:  0.475016880486158\n",
      "best epoch: 51 best recall:  0.48193787981093855\n",
      "[0.4662390276839973, 0.5869345037137069, 0.7059419311276165, 0.7996286293045239, 0.8713706954760297, 0.9247130317353139]\n",
      "epoch: 59 recall:  0.4662390276839973\n",
      "best epoch: 51 best recall:  0.48193787981093855\n",
      "[0.4777177582714382, 0.5952059419311276, 0.7094868332207968, 0.8094193112761647, 0.87457798784605, 0.925388251181634]\n",
      "epoch: 60 recall:  0.4777177582714382\n",
      "best epoch: 51 best recall:  0.48193787981093855\n",
      "[0.47113436866981767, 0.5953747467927076, 0.7052667116812964, 0.7965901417960837, 0.8668129642133694, 0.9177920324105334]\n",
      "epoch: 61 recall:  0.47113436866981767\n",
      "best epoch: 51 best recall:  0.48193787981093855\n",
      "[0.4647197839297772, 0.5827143821742066, 0.6985145172180959, 0.7938892640108035, 0.862930452397029, 0.9164415935178933]\n",
      "epoch: 62 recall:  0.4647197839297772\n",
      "best epoch: 51 best recall:  0.48193787981093855\n",
      "[0.4677582714382174, 0.5830519918973667, 0.700877785280216, 0.7965901417960837, 0.87474679270763, 0.9235313977042539]\n",
      "epoch: 63 recall:  0.4677582714382174\n",
      "best epoch: 51 best recall:  0.48193787981093855\n",
      "[0.4613436866981769, 0.5837272113436867, 0.6948008102633356, 0.7923700202565834, 0.8669817690749494, 0.9203241053342336]\n",
      "epoch: 64 recall:  0.4613436866981769\n",
      "best epoch: 51 best recall:  0.48193787981093855\n",
      "[0.462694125590817, 0.5784942606347063, 0.6910871033085753, 0.7913571910871033, 0.8674881836596894, 0.923869007427414]\n",
      "epoch: 65 recall:  0.462694125590817\n",
      "best epoch: 51 best recall:  0.48193787981093855\n",
      "[0.4734976367319379, 0.5938555030384876, 0.7083051991897367, 0.7976029709655638, 0.8683322079675895, 0.9181296421336934]\n",
      "epoch: 66 recall:  0.4734976367319379\n",
      "best epoch: 51 best recall:  0.48193787981093855\n",
      "[0.47012153950033764, 0.5975692099932478, 0.7072923700202566, 0.799459824442944, 0.8703578663065497, 0.9208305199189737]\n",
      "epoch: 67 recall:  0.47012153950033764\n",
      "best epoch: 51 best recall:  0.48193787981093855\n",
      "[0.46809588116137746, 0.5889601620526671, 0.7022282241728561, 0.7981093855503039, 0.8673193787981094, 0.924206617150574]\n",
      "epoch: 68 recall:  0.46809588116137746\n",
      "best epoch: 51 best recall:  0.48193787981093855\n",
      "[0.4709655638082377, 0.5914922349763673, 0.6959824442943957, 0.7910195813639432, 0.8681634031060095, 0.9233625928426739]\n",
      "epoch: 69 recall:  0.4709655638082377\n",
      "best epoch: 51 best recall:  0.48193787981093855\n",
      "[0.47670492910195816, 0.6036461850101283, 0.713200540175557, 0.8062120189061445, 0.8735651586765699, 0.9198176907494936]\n",
      "epoch: 70 recall:  0.47670492910195816\n",
      "best epoch: 51 best recall:  0.48193787981093855\n",
      "[0.4637069547602971, 0.5931802835921675, 0.6988521269412559, 0.7945644834571236, 0.8652937204591492, 0.9211681296421337]\n",
      "epoch: 71 recall:  0.4637069547602971\n",
      "best epoch: 51 best recall:  0.48193787981093855\n",
      "[0.4716407832545577, 0.5918298446995274, 0.7020594193112761, 0.7922012153950033, 0.8651249155975692, 0.9164415935178933]\n",
      "epoch: 72 recall:  0.4716407832545577\n",
      "best epoch: 51 best recall:  0.48193787981093855\n",
      "[0.48210668467251855, 0.6009453072248481, 0.7083051991897367, 0.7937204591492235, 0.8666441593517893, 0.9188048615800135]\n",
      "epoch: 73 recall:  0.48210668467251855\n",
      "best epoch: 73 best recall:  0.48210668467251855\n",
      "[0.48092505064145846, 0.599594868332208, 0.7074611748818366, 0.8011478730587441, 0.8713706954760297, 0.9235313977042539]\n",
      "epoch: 74 recall:  0.48092505064145846\n",
      "best epoch: 73 best recall:  0.48210668467251855\n",
      "[0.47906819716407834, 0.5965563808237677, 0.700708980418636, 0.7945644834571236, 0.8679945982444295, 0.9179608372721134]\n",
      "epoch: 75 recall:  0.47906819716407834\n",
      "best epoch: 73 best recall:  0.48210668467251855\n",
      "[0.46826468602295745, 0.587440918298447, 0.6963200540175557, 0.7925388251181634, 0.8700202565833896, 0.9213369345037137]\n",
      "epoch: 76 recall:  0.46826468602295745\n",
      "best epoch: 73 best recall:  0.48210668467251855\n",
      "[0.4647197839297772, 0.5909858203916273, 0.7032410533423362, 0.7950708980418636, 0.8649561107359892, 0.9189736664415935]\n",
      "epoch: 77 recall:  0.4647197839297772\n",
      "best epoch: 73 best recall:  0.48210668467251855\n",
      "[0.475523295070898, 0.5908170155300473, 0.7017218095881161, 0.7981093855503039, 0.8700202565833896, 0.9171168129642133]\n",
      "epoch: 78 recall:  0.475523295070898\n",
      "best epoch: 73 best recall:  0.48210668467251855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.48311951384199864, 0.6024645509790681, 0.7113436866981769, 0.8013166779203241, 0.8730587440918298, 0.9203241053342336]\n",
      "epoch: 79 recall:  0.48311951384199864\n",
      "best epoch: 79 best recall:  0.48311951384199864\n",
      "[0.45948683322079675, 0.5835584064821067, 0.6942943956785955, 0.7950708980418636, 0.8728899392302498, 0.9211681296421337]\n",
      "epoch: 80 recall:  0.45948683322079675\n",
      "best epoch: 79 best recall:  0.48311951384199864\n",
      "[0.4598244429439568, 0.5784942606347063, 0.6954760297096556, 0.7969277515192438, 0.8705266711681297, 0.9194800810263336]\n",
      "epoch: 81 recall:  0.4598244429439568\n",
      "best epoch: 79 best recall:  0.48311951384199864\n",
      "[0.474848075624578, 0.5930114787305875, 0.7045914922349764, 0.7964213369345037, 0.8658001350438893, 0.9203241053342336]\n",
      "epoch: 82 recall:  0.474848075624578\n",
      "best epoch: 79 best recall:  0.48311951384199864\n",
      "[0.47704253882511816, 0.5980756245779878, 0.7067859554355166, 0.7984469952734639, 0.8651249155975692, 0.9176232275489534]\n",
      "epoch: 83 recall:  0.47704253882511816\n",
      "best epoch: 79 best recall:  0.48311951384199864\n",
      "[0.475523295070898, 0.5963875759621877, 0.7003713706954761, 0.800135043889264, 0.8681634031060095, 0.9220121539500338]\n",
      "epoch: 84 recall:  0.475523295070898\n",
      "best epoch: 79 best recall:  0.48311951384199864\n",
      "[0.48058744091829847, 0.5972316002700878, 0.7045914922349764, 0.7940580688723835, 0.861748818365969, 0.9177920324105334]\n",
      "epoch: 85 recall:  0.48058744091829847\n",
      "best epoch: 79 best recall:  0.48311951384199864\n",
      "[0.45425388251181636, 0.5837272113436867, 0.6976704929101958, 0.7884875084402431, 0.862930452397029, 0.9171168129642133]\n",
      "epoch: 86 recall:  0.45425388251181636\n",
      "best epoch: 79 best recall:  0.48311951384199864\n",
      "[0.47214719783929776, 0.5896353814989872, 0.7017218095881161, 0.7943956785955435, 0.8647873058744092, 0.9213369345037137]\n",
      "epoch: 87 recall:  0.47214719783929776\n",
      "best epoch: 79 best recall:  0.48311951384199864\n",
      "[0.4669142471303174, 0.5842336259284268, 0.6981769074949359, 0.7964213369345037, 0.87508440243079, 0.9287643484132343]\n",
      "epoch: 88 recall:  0.4669142471303174\n",
      "best epoch: 79 best recall:  0.48311951384199864\n",
      "[0.4637069547602971, 0.5943619176232275, 0.7040850776502363, 0.7949020931802836, 0.8703578663065497, 0.9134031060094531]\n",
      "epoch: 89 recall:  0.4637069547602971\n",
      "best epoch: 79 best recall:  0.48311951384199864\n",
      "[0.4660702228224173, 0.5837272113436867, 0.699864956110736, 0.7955773126266037, 0.8708642808912896, 0.9189736664415935]\n",
      "epoch: 90 recall:  0.4660702228224173\n",
      "best epoch: 79 best recall:  0.48311951384199864\n",
      "[0.48058744091829847, 0.5957123565158676, 0.7059419311276165, 0.7952397029034436, 0.8622552329507089, 0.9155975692099932]\n",
      "epoch: 91 recall:  0.48058744091829847\n",
      "best epoch: 79 best recall:  0.48311951384199864\n",
      "[0.4738352464550979, 0.5960499662390277, 0.7133693450371371, 0.8041863605671843, 0.8762660364618501, 0.9235313977042539]\n",
      "epoch: 92 recall:  0.4738352464550979\n",
      "best epoch: 79 best recall:  0.48311951384199864\n",
      "[0.4552667116812964, 0.5808575286968265, 0.6932815665091154, 0.7900067521944631, 0.8622552329507089, 0.9152599594868333]\n",
      "epoch: 93 recall:  0.4552667116812964\n",
      "best epoch: 79 best recall:  0.48311951384199864\n",
      "[0.475523295070898, 0.5968939905469277, 0.7002025658338961, 0.7932140445644834, 0.8663065496286293, 0.9176232275489534]\n",
      "epoch: 94 recall:  0.475523295070898\n",
      "best epoch: 79 best recall:  0.48311951384199864\n",
      "[0.4618501012829169, 0.5849088453747467, 0.7015530047265361, 0.7903443619176233, 0.8641120864280891, 0.9174544226873734]\n",
      "epoch: 95 recall:  0.4618501012829169\n",
      "best epoch: 79 best recall:  0.48311951384199864\n",
      "[0.4687711006076975, 0.5919986495611074, 0.7030722484807562, 0.7987846049966238, 0.8740715732613099, 0.9267386900742741]\n",
      "epoch: 96 recall:  0.4687711006076975\n",
      "best epoch: 79 best recall:  0.48311951384199864\n",
      "[0.4637069547602971, 0.5810263335584065, 0.6968264686022958, 0.7922012153950033, 0.862424037812289, 0.9144159351789332]\n",
      "epoch: 97 recall:  0.4637069547602971\n",
      "best epoch: 79 best recall:  0.48311951384199864\n",
      "[0.474848075624578, 0.5945307224848075, 0.6983457123565159, 0.7984469952734639, 0.8678257933828494, 0.924037812288994]\n",
      "epoch: 98 recall:  0.474848075624578\n",
      "best epoch: 79 best recall:  0.48311951384199864\n",
      "[0.4707967589466577, 0.5901417960837272, 0.7029034436191762, 0.7937204591492235, 0.8669817690749494, 0.9181296421336934]\n",
      "epoch: 99 recall:  0.4707967589466577\n",
      "best epoch: 79 best recall:  0.48311951384199864\n"
     ]
    }
   ],
   "source": [
    "world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "\n",
    "train_tr = dataset.utils.make_transform(\n",
    "    is_train = True, \n",
    "    is_inception = (model == 'bn_inception')\n",
    ")\n",
    "\n",
    "ds_list = {\"CUB\": CUBirds, \"SOP\": SOP, \"Cars\": Cars, \"Inshop\": Inshop_Dataset}\n",
    "ds_class = ds_list[ds]\n",
    "ds_train = ds_class(path, \"train\", train_tr)\n",
    "\n",
    "sampler = UniqueClassSempler(\n",
    "    ds_train.ys, num_samples, local_rank, world_size\n",
    ")\n",
    "dl_train = DataLoader(\n",
    "    dataset=ds_train,\n",
    "    sampler=sampler,\n",
    "    batch_size=bs,\n",
    "    num_workers=workers,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "if model.find('resnet18')+1:\n",
    "    model = Resnet18(embedding_size=emb, pretrained=True, bn_freeze = bn_freeze,hyp_c = hyp_c, clip_r = clip_r).cuda().train() \n",
    "elif model.find('resnet34')+1:\n",
    "    model = Resnet34(embedding_size=emb, pretrained=True, bn_freeze = bn_freeze,hyp_c = hyp_c, clip_r = clip_r).cuda().train() \n",
    "elif model.find('resnet50')+1:\n",
    "    model = Resnet50(embedding_size=emb, pretrained=True, bn_freeze = bn_freeze,hyp_c = hyp_c, clip_r = clip_r) \n",
    "elif model.find('resnet101')+1:\n",
    "    model = Resnet101(embedding_size=emb, pretrained=True, bn_freeze = bn_freeze,hyp_c = hyp_c, clip_r = clip_r)\n",
    "\n",
    "if gpu_id == -1:\n",
    "    model = nn.DataParallel(model)\n",
    "    \n",
    "model.cuda().train()\n",
    "\n",
    "loss_f = partial(contrastive_loss, tau=t, hyp_c= hyp_c)\n",
    "\n",
    "get_emb_f = partial(\n",
    "    get_emb,\n",
    "    model=model,\n",
    "    ds=ds_class,\n",
    "    path=path,\n",
    "    num_workers=workers,\n",
    "    world_size=world_size,\n",
    ")\n",
    "optimizer = optim.AdamW(model.module.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=lr_decay_step, gamma = lr_decay_gamma)\n",
    "print(\"Training for {} epochs.\".format(ep))\n",
    "\n",
    "r0= evaluate(get_emb_f, ds, hyp_c=hyp_c)\n",
    "print(\"The recall before train: \", r0)\n",
    "\n",
    "losses_list = []\n",
    "best_recall= 0\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(0, ep):\n",
    "    model.train()\n",
    "    if bn_freeze:\n",
    "        modules = model.module.model.modules()\n",
    "        for m in modules: \n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                m.eval()\n",
    "\n",
    "    losses_per_epoch = []\n",
    "    sampler.set_epoch(epoch)\n",
    "    stats_ep = []\n",
    "    for x, y in dl_train:\n",
    "        y = y.view(len(y) // num_samples, num_samples)\n",
    "        assert (y[:, 0] == y[:, -1]).all()\n",
    "        s1 = y[:, 0].tolist()\n",
    "        assert len(set(s1)) == len(s1)\n",
    "\n",
    "        x = x.cuda(non_blocking=True)\n",
    "        p = model(x)\n",
    "        p = p.view(len(x) // num_samples, num_samples, emb)\n",
    "        loss = 0\n",
    "        for i in range(num_samples):\n",
    "            for j in range(num_samples):\n",
    "                if i != j:\n",
    "                    l, st = loss_f(p[:, i], p[:, j])\n",
    "                    loss += l\n",
    "                    stats_ep.append({**st, \"loss\": l.item()})\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 10)\n",
    "        optimizer.step()\n",
    "        \n",
    "    scheduler.step()        \n",
    "    rh= evaluate(get_emb_f, ds, hyp_c = hyp_c)\n",
    "    stats_ep = {k: np.mean([x[k] for x in stats_ep]) for k in stats_ep[0]}\n",
    "    stats_ep = {\"recall\": rh, **stats_ep}\n",
    "    if rh > best_recall :\n",
    "        best_recall = rh\n",
    "        best_epoch = epoch\n",
    "    print(\"epoch:\",epoch,\"recall: \", rh)\n",
    "    print(\"best epoch:\",best_epoch,\"best recall: \", best_recall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
